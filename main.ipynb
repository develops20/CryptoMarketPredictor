{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-cb-lgbm-xgb-nb153?scriptVersionId=251016383\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Step 1: Installations\n!pip install catboost lightgbm xgboost -q\n\n# Step 2: Imports\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport gc\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import pearsonr\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport time\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T12:11:25.54844Z","iopub.execute_input":"2025-07-17T12:11:25.548985Z","iopub.status.idle":"2025-07-17T12:11:34.798336Z","shell.execute_reply.started":"2025-07-17T12:11:25.548952Z","shell.execute_reply":"2025-07-17T12:11:34.797588Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Step 3: Class Definition\nclass CryptoMarketPredictor:\n    \"\"\"\n    An optimized pipeline using an ensemble of tree-based models (LGBM, XGBoost, CatBoost)\n    with rich feature engineering to predict crypto market movements.\n    \"\"\"\n    def __init__(self, top_features=100, top_X_features_to_preselect=30):\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None\n        self.models = {}\n        self.preselected_X_n_names = None\n\n    def optimize_memory(self, df):\n        \"\"\"Optimize DataFrame memory usage.\"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col not in ['timestamp', 'ID', 'label']:\n                df[col] = pd.to_numeric(df[col], downcast='float')\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"Clean DataFrame by handling inf, NaN values.\"\"\"\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        # Use forward-fill then backward-fill to propagate last valid observation\n        for col in df.columns:\n            if df[col].dtype == 'float32' or df[col].dtype == 'float64':\n                df[col] = df[col].ffill().bfill()\n        df.fillna(0, inplace=True) # Fill any remaining NaNs with 0\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate a proxy for the Relative Strength Index (RSI).\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n        rs = gain / (loss + 1e-10) # Add epsilon to avoid division by zero\n        rsi = 100 - (100 / (1 + rs))\n        return rsi.fillna(50) # Fill initial NaNs with 50 (neutral RSI)\n\n    def create_time_features(self, df):\n        \"\"\"Engineer a rich set of time-based features to capture market memory.\"\"\"\n        print(\"🛠️ Engineering rich time-based features...\")\n\n        # Basic price and imbalance features\n        df['mid_price'] = (df['ask_qty'] + df['bid_qty']) / 2 if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n        df['spread'] = df['ask_qty'] - df['bid_qty'] if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1e-10) \\\n                          if 'bid_qty' in df.columns and 'ask_qty' in df.columns else 0.0\n        \n        # Ensure 'volume' column exists before creating rolling features\n        cols_for_rolling = ['mid_price', 'volume', 'imbalance']\n        cols_for_rolling = [col for col in cols_for_rolling if col in df.columns]\n\n        # Rolling window features (MA and STD)\n        rolling_windows = [5, 10, 30, 60]\n        for col in cols_for_rolling:\n            for window in rolling_windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n\n        # Lag features\n        lag_periods = [1, 2, 5, 10]\n        cols_for_lags = ['mid_price', 'imbalance']\n        cols_for_lags = [col for col in cols_for_lags if col in df.columns]\n        for col in cols_for_lags:\n            for lag in lag_periods:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n        # Momentum feature - ensure 'mid_price' exists\n        if 'mid_price' in df.columns:\n            df['rsi_proxy_14'] = self.calculate_rsi_proxy(df['mid_price'], window=14)\n        else:\n            df['rsi_proxy_14'] = 50.0 # Default if mid_price is not available\n\n        # Final cleanup to handle NaNs introduced by rolling/shifting\n        df = self.clean_data(df)\n        print(f\"Feature engineering complete. Shape: {df.shape}\")\n        return df\n\n\n    def select_features(self, X_df, y_df):\n        \"\"\"Select top k features using f_regression to reduce dimensionality.\"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n        selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n        \n        X_df_clean = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n        y_df_clean = y_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n        if y_df_clean.nunique() == 1:\n            print(\"Warning: Target variable is constant. Skipping feature selection.\")\n            self.selected_features = X_df_clean.columns.tolist()\n            self.feature_selector = None\n            return X_df_clean.values\n            \n        selector.fit(X_df_clean, y_df_clean)\n        self.feature_selector = selector\n        self.selected_features = X_df_clean.columns[selector.get_support()].tolist()\n        print(f\"Selected {len(self.selected_features)} features.\")\n        return X_df_clean[self.selected_features].values\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance and print results.\"\"\"\n        mae = mean_absolute_error(y_true, y_pred)\n        \n        if np.std(y_true) == 0 or np.std(y_pred) == 0:\n            correlation = np.nan\n        else:\n            correlation, _ = pearsonr(y_true, y_pred)\n        \n        print(f\"📊 {model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train a LightGBM model.\"\"\"\n        print(\"\\nTraining LightGBM model...\")\n        params = {\n            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1500,\n            'learning_rate': 0.03, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n            'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1,\n            'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42\n        }\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n                  callbacks=[lgb.early_stopping(100, verbose=False)])\n        return model\n\n    def train_xgboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train an XGBoost model.\"\"\"\n        print(\"Training XGBoost model...\")\n        params = {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'mae',\n            'n_estimators': 1500,\n            'learning_rate': 0.03,\n            'tree_method': 'hist',\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'seed': 42,\n            'n_jobs': -1,\n        }\n        model = xgb.XGBRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n                  early_stopping_rounds=100, verbose=False) \n        return model\n\n    def train_catboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train a CatBoost model.\"\"\"\n        print(\"Training CatBoost model...\")\n        params = {\n            'objective': 'MAE', 'eval_metric': 'MAE', 'iterations': 1500,\n            'learning_rate': 0.03, 'random_seed': 42, 'logging_level': 'Silent',\n            'l2_leaf_reg': 3, 'bagging_temperature': 1,\n        }\n        model = cb.CatBoostRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n        return model\n\n    # ---------------------------------FIT METHOD----------------------------------------------------\n    def fit(self, train_data_raw):\n        \"\"\"Main training pipeline.\"\"\"\n        fit_start_time = time.time()\n        print(\"🚀 Starting training pipeline...\")\n\n        # Ensure 'timestamp' exists and is datetime\n        if 'timestamp' not in train_data_raw.columns:\n            if train_data_raw.index.name == 'timestamp':\n                train_data_raw.reset_index(inplace=True)\n            elif 'index' in train_data_raw.columns:\n                 train_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n            else:\n                raise KeyError(\"The 'timestamp' column is missing from the DataFrame and cannot be inferred.\")\n        \n        train_data_raw['timestamp'] = pd.to_datetime(train_data_raw['timestamp'])\n\n        # --- 1. Filter data to the most recent 3 months ---\n        print(\"Filtering data to the last 3 months for relevance and speed...\")\n        max_date_full = train_data_raw['timestamp'].max()\n        three_months_prior = max_date_full - pd.DateOffset(months=3)\n        train_df_filtered = train_data_raw[train_data_raw['timestamp'] >= three_months_prior].copy()\n        \n        if train_df_filtered.empty:\n            raise ValueError(\"Training DataFrame is empty after filtering to the last 3 months. Adjust filter or provide more data.\")\n\n        print(f\"Training on data from {train_df_filtered['timestamp'].min().date()} to {train_df_filtered['timestamp'].max().date()}. Shape: {train_df_filtered.shape}\")\n        \n        # Ensure train_df is sorted by timestamp after filtering\n        print(\"Sorting train_df by timestamp for chronological splitting...\")\n        train_df_filtered.sort_values(by='timestamp', inplace=True)\n        train_df_filtered.reset_index(drop=True, inplace=True) # Reset index after sort for clean slicing\n\n        # Now, use this filtered and sorted DataFrame for all subsequent steps\n        train_df = train_df_filtered \n        del train_data_raw, train_df_filtered; gc.collect() # Free up memory from original and temp filtered df\n\n        # --- 1a. More intelligent preselection of the anonymous features ---\n        sample_df = train_df.sample(n=min(50000, len(train_df)), random_state=42)\n        X_n_cols_raw = [c for c in sample_df.columns if c.startswith('X')]\n        X_raw_sample = sample_df[X_n_cols_raw].copy()\n        X_raw_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n        X_raw_sample.fillna(0, inplace=True)\n        y_raw_sample = sample_df['label'].fillna(0)\n\n        pre_selector = SelectKBest(score_func=f_regression, k=self.top_X_features_to_preselect)\n        if y_raw_sample.nunique() == 1:\n            print(\"Warning: Sample target variable is constant. Preselecting all 'X' features.\")\n            self.preselected_X_n_names = X_raw_sample.columns.tolist()\n        else:\n            pre_selector.fit(X_raw_sample, y_raw_sample)\n            self.preselected_X_n_names = X_raw_sample.columns[pre_selector.get_support()].tolist()\n\n        print(f\"Found the top {len(self.preselected_X_n_names)} 'X' features: {self.preselected_X_n_names}\")\n        del sample_df, X_raw_sample, y_raw_sample, pre_selector; gc.collect()\n\n        # --- 2. Preprocessing and Feature Engineering ---\n        cols_to_use = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label'] + self.preselected_X_n_names\n        cols_to_use = [col for col in cols_to_use if col in train_df.columns]\n\n        print(\"\\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\")\n        print(f\"Using {len(cols_to_use)} columns to generate features\")\n        print(\"-----------------------------------------------------\\n\")\n\n        train_df = train_df[cols_to_use]\n        train_df = self.optimize_memory(train_df)\n        train_df = self.create_time_features(train_df)\n\n        print(\"\\n--- 2. All Features After Engineering Step ---\")\n        engineered_features_list = train_df.columns.tolist()\n        print(f\"Created {len(engineered_features_list)} total features: {engineered_features_list}\")\n        print(\"------------------------------------------\\n\")\n\n        # --- 3. Feature Selection ---\n        feature_cols = [c for c in train_df.columns if c not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols]\n        y_df_aligned = train_df['label'] # Align y_df with the current train_df after all processing\n\n        X_selected_array = self.select_features(X_df, y_df_aligned)\n        \n        # --- 4. Scaling and Time-Based Validation Split (using last N rows) ---\n        print(\"Splitting data into training and validation sets using last N rows for validation...\")\n        X_scaled = self.scaler.fit_transform(X_selected_array)\n        \n        total_samples = len(X_scaled)\n        min_samples_required = 2 # LGBM requires at least 2 samples\n        val_percentage = 0.2 # Use 20% of data for validation\n\n        # Calculate split point: last 20% of data for validation\n        val_size = max(min_samples_required, int(total_samples * val_percentage))\n        train_size = total_samples - val_size\n\n        # Ensure minimums for percentage split\n        if train_size < min_samples_required:\n            raise ValueError(f\"Not enough data to create valid training set ({train_size} samples) after reserving {val_size} for validation. Total samples: {total_samples}\")\n        if val_size < min_samples_required:\n            raise ValueError(f\"Not enough data to create valid validation set ({val_size} samples). Total samples: {total_samples}\")\n\n        X_train, X_val = X_scaled[:train_size], X_scaled[train_size:]\n        y_train, y_val = y_df_aligned.iloc[:train_size], y_df_aligned.iloc[train_size:]\n\n        print(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")\n        del X_df, y_df_aligned, X_selected_array, X_scaled, train_df; gc.collect()\n\n        # --- 5. Train Models ---\n        self.models['lgb'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n        self.models['xgb'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n        self.models['cat'] = self.train_catboost(X_train, y_train, X_val, y_val)\n\n        # --- 6. Evaluate Ensemble on Validation Set ---\n        print(\"\\n--- Validation Set Evaluation ---\")\n        val_predictions = {}\n        for name, model in self.models.items():\n            val_predictions[name] = model.predict(X_val)\n            self.evaluate_model(y_val, val_predictions[name], name.upper())\n\n        ensemble_pred = np.mean([val_predictions['lgb'], val_predictions['xgb'], val_predictions['cat']], axis=0)\n        self.evaluate_model(y_val, ensemble_pred, \"ENSEMBLE\")\n\n        print(\"\\n✅ Training pipeline complete.\")\n        return self\n\n    def predict(self, test_data_raw):\n        \"\"\"\n        Generate predictions for test data in chunks to manage memory.\n        \"\"\"\n        predict_start_time = time.time()\n        print(\"\\nGenerating predictions in chunks to manage memory...\")\n        if not self.models:\n            raise ValueError(\"Models must be trained first. Call fit().\")\n        if not self.selected_features:\n             raise ValueError(\"Features must be selected first. Call fit().\")\n        if not self.scaler:\n            raise ValueError(\"Scaler must be fitted first. Call fit().\")\n\n        chunk_size = 100000\n        num_chunks = int(np.ceil(len(test_data_raw) / chunk_size))\n        all_predictions = []\n        overlap = 60\n\n        # Ensure 'timestamp' is correctly identified in test_data_raw\n        if 'timestamp' not in test_data_raw.columns:\n            if test_data_raw.index.name == 'timestamp':\n                test_data_raw.reset_index(inplace=True)\n            elif 'index' in test_data_raw.columns:\n                test_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n            else:\n                raise KeyError(\"The 'timestamp' column is missing from the test DataFrame and cannot be inferred.\")\n\n        # Store the original IDs before chunking, as predictions will be generated in order\n        original_test_ids = test_data_raw['ID'].copy()\n\n        for i in range(num_chunks):\n            print(f\"--> Processing chunk {i+1}/{num_chunks}...\")\n\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, len(test_data_raw))\n            start_with_overlap = max(0, start_idx - overlap) \n            chunk = test_data_raw.iloc[start_with_overlap:end_idx].copy()\n            \n            chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n\n            base_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n            cols_to_use_in_chunk = [col for col in base_cols + self.preselected_X_n_names if col in chunk.columns]\n            \n            if not cols_to_use_in_chunk:\n                raise ValueError(\"No valid columns found in test chunk for feature engineering.\")\n\n            chunk = chunk[cols_to_use_in_chunk]\n\n            chunk_featured = self.create_time_features(chunk)\n\n            chunk_featured = chunk_featured.iloc[start_idx - start_with_overlap : ]\n\n            missing_features = [f for f in self.selected_features if f not in chunk_featured.columns]\n            if missing_features:\n                print(f\"Warning: Missing features in test chunk: {missing_features}. Filling with 0.\")\n                for mf in missing_features:\n                    chunk_featured[mf] = 0.0\n\n            X_test_df = chunk_featured[self.selected_features]\n            X_test_scaled = self.scaler.transform(X_test_df)\n            X_test_scaled = np.nan_to_num(X_test_scaled)\n\n            chunk_predictions = []\n            for name, model in self.models.items():\n                pred = model.predict(X_test_scaled)\n                chunk_predictions.append(pred)\n\n            ensemble_prediction = np.mean(chunk_predictions, axis=0)\n            all_predictions.append(ensemble_prediction)\n\n        final_predictions = np.concatenate(all_predictions)\n        final_predictions = np.nan_to_num(final_predictions, nan=0.0)\n\n        predict_duration = time.time() - predict_start_time\n        print(f\"Generated {len(final_predictions)} predictions in {predict_duration:.2f} seconds.\")\n        return final_predictions, original_test_ids","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T12:11:44.619268Z","iopub.execute_input":"2025-07-17T12:11:44.619959Z","iopub.status.idle":"2025-07-17T12:11:44.668272Z","shell.execute_reply.started":"2025-07-17T12:11:44.61993Z","shell.execute_reply":"2025-07-17T12:11:44.667532Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Cell 3 (Main function with the crucial ID handling fix and print statements)\ndef main():\n    \"\"\"\n    A wrapper function to define all variables locally and allow for cleanup.\n    \"\"\"\n    print(\"Loading data...\")\n    train_full_raw = None\n    test_full_raw = None\n    predictor = None\n    submission_df = None\n\n    try:\n        # 1. LOAD DATA\n        # Load train data\n        train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n        \n        # Load test data\n        test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n        \n        # --- NEW ROBUST ID/TIMESTAMP HANDLING ---\n        def ensure_id_and_timestamp(df, df_name):\n            df = df.reset_index()\n\n            timestamp_col = None\n            for col in df.columns:\n                try:\n                    temp_series = pd.to_datetime(df[col], errors='coerce')\n                    if not temp_series.isna().all() and col not in ['index', 'ID']:\n                        timestamp_col = col\n                        break\n                except Exception:\n                    pass\n\n            if timestamp_col and timestamp_col != 'timestamp':\n                df.rename(columns={timestamp_col: 'timestamp'}, inplace=True)\n            elif not timestamp_col and 'timestamp' not in df.columns:\n                raise KeyError(f\"The 'timestamp' column is missing from the {df_name} DataFrame and cannot be inferred.\")\n\n            id_col = None\n            if 'ID' in df.columns:\n                id_col = 'ID'\n            else:\n                potential_id_cols = [col for col in df.columns if col not in ['timestamp'] and df[col].dtype in ['int64', 'int32', 'Int64']]\n                \n                level_id_col = None\n                for p_id_col in potential_id_cols:\n                    if p_id_col.startswith('level_'):\n                        level_id_col = p_id_col\n                        break\n                \n                if level_id_col:\n                    id_col = level_id_col\n                    df.rename(columns={id_col: 'ID'}, inplace=True)\n                elif len(potential_id_cols) >= 1:\n                    id_col = potential_id_cols[0]\n                    df.rename(columns={id_col: 'ID'}, inplace=True)\n                else:\n                    df['ID'] = df.index\n                    print(f\"Warning: 'ID' column not found in {df_name}. Using DataFrame index as 'ID'. This might cause submission issues.\")\n\n            df['timestamp'] = pd.to_datetime(df['timestamp'])\n            \n            if not pd.api.types.is_integer_dtype(df['ID']):\n                try:\n                    df['ID'] = pd.to_numeric(df['ID'], errors='coerce').astype('Int64') \n                    if df['ID'].isnull().any():\n                        print(f\"Warning: NaNs introduced in 'ID' column of {df_name} during conversion to integer. Filling with sequential numbers.\")\n                        df.loc[df['ID'].isnull(), 'ID'] = np.arange(df['ID'].isnull().sum()) \n                except Exception:\n                    print(f\"Warning: Could not convert 'ID' column to integer in {df_name}. Keeping original type.\")\n\n            return df\n\n        train_full_raw = ensure_id_and_timestamp(train_full_raw, 'train')\n        test_full_raw = ensure_id_and_timestamp(test_full_raw, 'test')\n        # --- END NEW ROBUST ID/TIMESTAMP HANDLING ---\n\n\n        print(f\"\\nTrain shape: {train_full_raw.shape}\")\n        print(f\"Test shape: {test_full_raw.shape}\")\n\n        # 2. INITIALIZE AND TRAIN MODEL\n        predictor = CryptoMarketPredictor(top_features=80, top_X_features_to_preselect=25)\n        predictor.fit(train_full_raw)\n\n        # 3. GENERATE PREDICTIONS\n        predictions, original_test_ids = predictor.predict(test_full_raw)\n        \n        # Final sanity checks on predictions before saving\n        print(\"\\nPerforming final checks on generated predictions...\")\n        predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n        predictions = np.clip(predictions, -1.0, 1.0) # Adjust min/max if your target has a different range.\n        print(f\"Predictions min: {np.min(predictions):.4f}, max: {np.max(predictions):.4f}\")\n        print(f\"Number of NaNs after final check: {np.isnan(predictions).sum()}\")\n        print(f\"Number of Infs after final check: {np.isinf(predictions).sum()}\")\n\n        # 4. CREATE SUBMISSION\n        submission_df = pd.DataFrame({'ID': original_test_ids, 'Prediction': predictions})\n        submission_df.to_csv('submission.csv', index=False)\n        print(\"\\nSubmission file 'submission.csv' created successfully.\")\n        \n        # --- Print first 5 and last 5 rows of submission_df ---\n        print(\"\\nFirst 5 rows of submission:\")\n        print(submission_df.head())\n        print(\"\\nLast 5 rows of submission:\")\n        print(submission_df.tail())\n        # --- END Print ---\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc()\n\n    finally:\n        # 5. CLEANUP\n        print(\"\\n🧹 Cleaning up variables to free memory...\")\n        del train_full_raw\n        del test_full_raw\n        del predictor\n        del submission_df\n        gc.collect()\n        print(\"Cleanup complete. Ready for another run.\")\n\n# --- This line calls the main function to start the pipeline ---\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T12:11:50.193028Z","iopub.execute_input":"2025-07-17T12:11:50.19355Z","iopub.status.idle":"2025-07-17T12:17:39.06351Z","shell.execute_reply.started":"2025-07-17T12:11:50.193521Z","shell.execute_reply":"2025-07-17T12:17:39.062801Z"}},"outputs":[{"name":"stdout","text":"Loading data...\nWarning: 'ID' column not found in train. Using DataFrame index as 'ID'. This might cause submission issues.\n\nTrain shape: (525886, 788)\nTest shape: (538150, 787)\n🚀 Starting training pipeline...\nFiltering data to the last 3 months for relevance and speed...\nTraining on data from 1970-01-01 to 1970-01-01. Shape: (525886, 788)\nSorting train_df by timestamp for chronological splitting...\nFound the top 25 'X' features: ['X19', 'X20', 'X21', 'X22', 'X27', 'X28', 'X29', 'X218', 'X219', 'X287', 'X289', 'X291', 'X293', 'X295', 'X297', 'X298', 'X299', 'X465', 'X466', 'X614', 'X744', 'X751', 'X752', 'X753', 'X759']\n\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\nUsing 31 columns to generate features\n-----------------------------------------------------\n\nMemory usage before optimization: 124.38 MB\nMemory usage after optimization: 72.22 MB\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (525886, 67)\n\n--- 2. All Features After Engineering Step ---\nCreated 67 total features: ['timestamp', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label', 'X19', 'X20', 'X21', 'X22', 'X27', 'X28', 'X29', 'X218', 'X219', 'X287', 'X289', 'X291', 'X293', 'X295', 'X297', 'X298', 'X299', 'X465', 'X466', 'X614', 'X744', 'X751', 'X752', 'X753', 'X759', 'mid_price', 'spread', 'imbalance', 'mid_price_ma_5', 'mid_price_std_5', 'mid_price_ma_10', 'mid_price_std_10', 'mid_price_ma_30', 'mid_price_std_30', 'mid_price_ma_60', 'mid_price_std_60', 'volume_ma_5', 'volume_std_5', 'volume_ma_10', 'volume_std_10', 'volume_ma_30', 'volume_std_30', 'volume_ma_60', 'volume_std_60', 'imbalance_ma_5', 'imbalance_std_5', 'imbalance_ma_10', 'imbalance_std_10', 'imbalance_ma_30', 'imbalance_std_30', 'imbalance_ma_60', 'imbalance_std_60', 'mid_price_lag_1', 'mid_price_lag_2', 'mid_price_lag_5', 'mid_price_lag_10', 'imbalance_lag_1', 'imbalance_lag_2', 'imbalance_lag_5', 'imbalance_lag_10', 'rsi_proxy_14']\n------------------------------------------\n\nSelecting top 80 features from 65 features...\nSelected 65 features.\nSplitting data into training and validation sets using last N rows for validation...\nTrain set size: 420709, Validation set size: 105177\n\nTraining LightGBM model...\nTraining XGBoost model...\nTraining CatBoost model...\n\n--- Validation Set Evaluation ---\n📊 LGB - MAE: 0.4410, Pearson Correlation: 0.5986\n📊 XGB - MAE: 0.4006, Pearson Correlation: 0.7828\n📊 CAT - MAE: 0.4789, Pearson Correlation: 0.5096\n📊 ENSEMBLE - MAE: 0.4356, Pearson Correlation: 0.7161\n\n✅ Training pipeline complete.\n\nGenerating predictions in chunks to manage memory...\n--> Processing chunk 1/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (100000, 66)\n--> Processing chunk 2/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 66)\n--> Processing chunk 3/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 66)\n--> Processing chunk 4/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 66)\n--> Processing chunk 5/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 66)\n--> Processing chunk 6/6...\n🛠️ Engineering rich time-based features...\nFeature engineering complete. Shape: (38210, 66)\nGenerated 538150 predictions in 33.12 seconds.\n\nPerforming final checks on generated predictions...\nPredictions min: -1.0000, max: 1.0000\nNumber of NaNs after final check: 0\nNumber of Infs after final check: 0\n\nSubmission file 'submission.csv' created successfully.\n\nFirst 5 rows of submission:\n   ID  Prediction\n0   1    0.166891\n1   2   -0.391753\n2   3   -0.461615\n3   4   -0.021867\n4   5   -0.499352\n\nLast 5 rows of submission:\n            ID  Prediction\n538145  538146   -0.228261\n538146  538147   -0.338181\n538147  538148   -0.647834\n538148  538149   -0.223610\n538149  538150    0.241945\n\n🧹 Cleaning up variables to free memory...\nCleanup complete. Ready for another run.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}