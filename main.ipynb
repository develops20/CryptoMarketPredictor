{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-cb-lgbm-xgb-nb153?scriptVersionId=250977187\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"26d5fb0f","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-07-17T08:00:22.538782Z","iopub.status.busy":"2025-07-17T08:00:22.538503Z","iopub.status.idle":"2025-07-17T08:00:39.041146Z","shell.execute_reply":"2025-07-17T08:00:39.040503Z"},"papermill":{"duration":16.508069,"end_time":"2025-07-17T08:00:39.042798","exception":false,"start_time":"2025-07-17T08:00:22.534729","status":"completed"},"tags":[]},"outputs":[],"source":["# Step 1: Installations\n","!pip install catboost lightgbm xgboost -q\n","\n","# Step 2: Imports\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","import gc\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.feature_selection import SelectKBest, f_regression\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import pearsonr\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cb\n","import time\n"]},{"cell_type":"code","execution_count":2,"id":"4b09522c","metadata":{"execution":{"iopub.execute_input":"2025-07-17T08:00:39.049519Z","iopub.status.busy":"2025-07-17T08:00:39.0487Z","iopub.status.idle":"2025-07-17T08:00:39.086485Z","shell.execute_reply":"2025-07-17T08:00:39.085923Z"},"papermill":{"duration":0.042019,"end_time":"2025-07-17T08:00:39.087571","exception":false,"start_time":"2025-07-17T08:00:39.045552","status":"completed"},"tags":[]},"outputs":[],"source":["# Step 3: Class Definition\n","class CryptoMarketPredictor:\n","    \"\"\"\n","    An optimized pipeline using an ensemble of tree-based models (LGBM, XGBoost, CatBoost)\n","    with rich feature engineering to predict crypto market movements.\n","    \"\"\"\n","    def __init__(self, top_features=100, top_X_features_to_preselect=30):\n","        self.top_features = top_features\n","        self.top_X_features_to_preselect = top_X_features_to_preselect\n","        self.scaler = RobustScaler()\n","        self.feature_selector = None\n","        self.selected_features = None\n","        self.models = {}\n","        self.preselected_X_n_names = None\n","\n","    def optimize_memory(self, df):\n","        \"\"\"Optimize DataFrame memory usage.\"\"\"\n","        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","        for col in df.select_dtypes(include=[np.number]).columns:\n","            if col not in ['timestamp', 'ID', 'label']:\n","                df[col] = pd.to_numeric(df[col], downcast='float')\n","        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","        return df\n","\n","    def clean_data(self, df):\n","        \"\"\"Clean DataFrame by handling inf, NaN values.\"\"\"\n","        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        # Use forward-fill then backward-fill to propagate last valid observation\n","        for col in df.columns:\n","            if df[col].dtype == 'float32' or df[col].dtype == 'float64':\n","                df[col] = df[col].ffill().bfill()\n","        df.fillna(0, inplace=True) # Fill any remaining NaNs with 0\n","        return df\n","\n","    def calculate_rsi_proxy(self, prices_series, window=14):\n","        \"\"\"Calculate a proxy for the Relative Strength Index (RSI).\"\"\"\n","        delta = prices_series.diff()\n","        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n","        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n","        rs = gain / (loss + 1e-10) # Add epsilon to avoid division by zero\n","        rsi = 100 - (100 / (1 + rs))\n","        return rsi.fillna(50) # Fill initial NaNs with 50 (neutral RSI)\n","\n","    def create_time_features(self, df):\n","        \"\"\"Engineer a rich set of time-based features to capture market memory.\"\"\"\n","        print(\"🛠️ Engineering rich time-based features...\")\n","\n","        # Basic price and imbalance features\n","        df['mid_price'] = (df['ask_qty'] + df['bid_qty']) / 2 if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n","        df['spread'] = df['ask_qty'] - df['bid_qty'] if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n","        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1e-10) \\\n","                          if 'bid_qty' in df.columns and 'ask_qty' in df.columns else 0.0\n","        \n","        # Ensure 'volume' column exists before creating rolling features\n","        cols_for_rolling = ['mid_price', 'volume', 'imbalance']\n","        cols_for_rolling = [col for col in cols_for_rolling if col in df.columns]\n","\n","        # Rolling window features (MA and STD)\n","        rolling_windows = [5, 10, 30, 60]\n","        for col in cols_for_rolling:\n","            for window in rolling_windows:\n","                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n","                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n","\n","        # Lag features\n","        lag_periods = [1, 2, 5, 10]\n","        cols_for_lags = ['mid_price', 'imbalance']\n","        cols_for_lags = [col for col in cols_for_lags if col in df.columns]\n","        for col in cols_for_lags:\n","            for lag in lag_periods:\n","                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n","\n","        # Momentum feature - ensure 'mid_price' exists\n","        if 'mid_price' in df.columns:\n","            df['rsi_proxy_14'] = self.calculate_rsi_proxy(df['mid_price'], window=14)\n","        else:\n","            df['rsi_proxy_14'] = 50.0 # Default if mid_price is not available\n","\n","        # Final cleanup to handle NaNs introduced by rolling/shifting\n","        df = self.clean_data(df)\n","        print(f\"Feature engineering complete. Shape: {df.shape}\")\n","        return df\n","\n","\n","    def select_features(self, X_df, y_df):\n","        \"\"\"Select top k features using f_regression to reduce dimensionality.\"\"\"\n","        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n","        n_features_to_select = min(self.top_features, X_df.shape[1])\n","        selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n","        \n","        X_df_clean = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n","        y_df_clean = y_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n","\n","        if y_df_clean.nunique() == 1:\n","            print(\"Warning: Target variable is constant. Skipping feature selection.\")\n","            self.selected_features = X_df_clean.columns.tolist()\n","            self.feature_selector = None\n","            return X_df_clean.values\n","            \n","        selector.fit(X_df_clean, y_df_clean)\n","        self.feature_selector = selector\n","        self.selected_features = X_df_clean.columns[selector.get_support()].tolist()\n","        print(f\"Selected {len(self.selected_features)} features.\")\n","        return X_df_clean[self.selected_features].values\n","\n","    def evaluate_model(self, y_true, y_pred, model_name):\n","        \"\"\"Evaluate model performance and print results.\"\"\"\n","        mae = mean_absolute_error(y_true, y_pred)\n","        \n","        if np.std(y_true) == 0 or np.std(y_pred) == 0:\n","            correlation = np.nan\n","        else:\n","            correlation, _ = pearsonr(y_true, y_pred)\n","        \n","        print(f\"📊 {model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n","        return correlation\n","\n","    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train a LightGBM model.\"\"\"\n","        print(\"\\nTraining LightGBM model...\")\n","        params = {\n","            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1500,\n","            'learning_rate': 0.03, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n","            'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1,\n","            'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42\n","        }\n","        model = lgb.LGBMRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n","                  callbacks=[lgb.early_stopping(100, verbose=False)])\n","        return model\n","\n","    def train_xgboost(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train an XGBoost model.\"\"\"\n","        print(\"Training XGBoost model...\")\n","        params = {\n","            'objective': 'reg:squarederror',\n","            'eval_metric': 'mae',\n","            'n_estimators': 1500,\n","            'learning_rate': 0.03,\n","            'tree_method': 'hist',\n","            'subsample': 0.8,\n","            'colsample_bytree': 0.8,\n","            'seed': 42,\n","            'n_jobs': -1,\n","        }\n","        model = xgb.XGBRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n","                  early_stopping_rounds=100, verbose=False) \n","        return model\n","\n","    def train_catboost(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train a CatBoost model.\"\"\"\n","        print(\"Training CatBoost model...\")\n","        params = {\n","            'objective': 'MAE', 'eval_metric': 'MAE', 'iterations': 1500,\n","            'learning_rate': 0.03, 'random_seed': 42, 'logging_level': 'Silent',\n","            'l2_leaf_reg': 3, 'bagging_temperature': 1,\n","        }\n","        model = cb.CatBoostRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","        return model\n","\n","    # ---------------------------------FIT METHOD----------------------------------------------------\n","    def fit(self, train_data_raw):\n","        \"\"\"Main training pipeline.\"\"\"\n","        fit_start_time = time.time()\n","        print(\"🚀 Starting training pipeline...\")\n","\n","        # Ensure 'timestamp' exists and is datetime\n","        if 'timestamp' not in train_data_raw.columns:\n","            if train_data_raw.index.name == 'timestamp':\n","                train_data_raw.reset_index(inplace=True)\n","            elif 'index' in train_data_raw.columns:\n","                 train_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n","            else:\n","                raise KeyError(\"The 'timestamp' column is missing from the DataFrame and cannot be inferred.\")\n","        \n","        train_data_raw['timestamp'] = pd.to_datetime(train_data_raw['timestamp'])\n","\n","        # --- 1. Filter data to the most recent 3 months ---\n","        print(\"Filtering data to the last 3 months for relevance and speed...\")\n","        max_date = train_data_raw['timestamp'].max()\n","        three_months_prior = max_date - pd.DateOffset(months=3)\n","        train_df = train_data_raw[train_data_raw['timestamp'] >= three_months_prior].copy()\n","        \n","        # Check if train_df became empty after filtering\n","        if train_df.empty:\n","            raise ValueError(\"Training DataFrame is empty after filtering to the last 3 months. Adjust filter or provide more data.\")\n","\n","        print(f\"Training on data from {three_months_prior.date()} to {max_date.date()}. Shape: {train_df.shape}\")\n","        del train_data_raw; gc.collect()\n","\n","        # --- 1a. More intelligent preselection of the anonymous features ---\n","        sample_df = train_df.sample(n=min(50000, len(train_df)), random_state=42)\n","        X_n_cols_raw = [c for c in sample_df.columns if c.startswith('X')]\n","        X_raw_sample = sample_df[X_n_cols_raw].copy()\n","        X_raw_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        X_raw_sample.fillna(0, inplace=True)\n","        y_raw_sample = sample_df['label'].fillna(0)\n","\n","        pre_selector = SelectKBest(score_func=f_regression, k=self.top_X_features_to_preselect)\n","        if y_raw_sample.nunique() == 1:\n","            print(\"Warning: Sample target variable is constant. Preselecting all 'X' features.\")\n","            self.preselected_X_n_names = X_raw_sample.columns.tolist()\n","        else:\n","            pre_selector.fit(X_raw_sample, y_raw_sample)\n","            self.preselected_X_n_names = X_raw_sample.columns[pre_selector.get_support()].tolist()\n","\n","        print(f\"Found the top {len(self.preselected_X_n_names)} 'X' features: {self.preselected_X_n_names}\")\n","        del sample_df, X_raw_sample, y_raw_sample, pre_selector; gc.collect()\n","\n","        # --- 2. Preprocessing and Feature Engineering ---\n","        cols_to_use = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label'] + self.preselected_X_n_names\n","        cols_to_use = [col for col in cols_to_use if col in train_df.columns]\n","\n","        print(\"\\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\")\n","        print(f\"Using {len(cols_to_use)} columns to generate features\")\n","        print(\"-----------------------------------------------------\\n\")\n","\n","        train_df = train_df[cols_to_use]\n","        train_df = self.optimize_memory(train_df)\n","        train_df = self.create_time_features(train_df)\n","\n","        print(\"\\n--- 2. All Features After Engineering Step ---\")\n","        engineered_features_list = train_df.columns.tolist()\n","        print(f\"Created {len(engineered_features_list)} total features: {engineered_features_list}\")\n","        print(\"------------------------------------------\\n\")\n","\n","        # --- 3. Feature Selection ---\n","        feature_cols = [c for c in train_df.columns if c not in ['timestamp', 'label']]\n","        X_df = train_df[feature_cols]\n","        y_df = train_df['label']\n","\n","        X_selected_array = self.select_features(X_df, y_df)\n","        \n","        # --- 4. Scaling and Time-Based Validation Split ---\n","        print(\"Splitting data into training and validation sets based on time...\")\n","        X_scaled = self.scaler.fit_transform(X_selected_array)\n","\n","        # Calculate split point for validation set (last month)\n","        split_date = train_df['timestamp'].max() - pd.DateOffset(months=1)\n","        # Find the index where the timestamp is less than the split_date\n","        val_start_pos = train_df[train_df['timestamp'] < split_date].shape[0]\n","        \n","        # Ensure minimum samples for training and validation\n","        min_samples_required = 2 # LGBM requires at least 2 samples\n","        \n","        # If the calculated val_start_pos is too small (e.g., 0 or 1),\n","        # or if it's so large that the validation set would be empty,\n","        # adjust it to ensure both train and val sets have enough samples.\n","        total_samples = len(X_scaled)\n","\n","        if total_samples < min_samples_required * 2: # Not enough data for both train and val\n","            raise ValueError(f\"Not enough data ({total_samples} samples) to create valid train/validation split with at least {min_samples_required} samples in each.\")\n","\n","        # Adjust val_start_pos if it's too small for training or leaves no room for validation\n","        if val_start_pos < min_samples_required:\n","            print(f\"Warning: Time-based split resulted in too few training samples ({val_start_pos}). Adjusting split to ensure minimum training size.\")\n","            val_start_pos = min_samples_required\n","        \n","        # Ensure there are enough samples left for the validation set\n","        if total_samples - val_start_pos < min_samples_required:\n","            print(f\"Warning: Time-based split resulted in too few validation samples ({total_samples - val_start_pos}). Adjusting split to ensure minimum validation size.\")\n","            val_start_pos = total_samples - min_samples_required\n","        \n","        # Final check after adjustments\n","        if val_start_pos < min_samples_required or (total_samples - val_start_pos) < min_samples_required:\n","            raise ValueError(f\"Failed to create valid train/validation split with at least {min_samples_required} samples in each after adjustments. Consider increasing data or reducing filter.\")\n","\n","\n","        X_train, X_val = X_scaled[:val_start_pos], X_scaled[val_start_pos:]\n","        y_train, y_val = y_df.iloc[:val_start_pos], y_df.iloc[val_start_pos:]\n","\n","        print(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")\n","        del X_df, y_df, X_selected_array, X_scaled, train_df; gc.collect()\n","\n","        # --- 5. Train Models ---\n","        # Ensure X_train and X_val are not empty before training\n","        if X_train.shape[0] < min_samples_required or X_val.shape[0] < min_samples_required:\n","            raise ValueError(f\"Train or Validation set has fewer than {min_samples_required} samples after splitting. Cannot train models.\")\n","\n","        self.models['lgb'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n","        self.models['xgb'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n","        self.models['cat'] = self.train_catboost(X_train, y_train, X_val, y_val)\n","\n","        # --- 6. Evaluate Ensemble on Validation Set ---\n","        print(\"\\n--- Validation Set Evaluation ---\")\n","        val_predictions = {}\n","        for name, model in self.models.items():\n","            val_predictions[name] = model.predict(X_val)\n","            self.evaluate_model(y_val, val_predictions[name], name.upper())\n","\n","        ensemble_pred = np.mean([val_predictions['lgb'], val_predictions['xgb'], val_predictions['cat']], axis=0)\n","        self.evaluate_model(y_val, ensemble_pred, \"ENSEMBLE\")\n","\n","        print(\"\\n✅ Training pipeline complete.\")\n","        return self\n","\n","    def predict(self, test_data_raw):\n","        \"\"\"\n","        Generate predictions for test data in chunks to manage memory.\n","        \"\"\"\n","        predict_start_time = time.time()\n","        print(\"\\nGenerating predictions in chunks to manage memory...\")\n","        if not self.models:\n","            raise ValueError(\"Models must be trained first. Call fit().\")\n","        if not self.selected_features:\n","             raise ValueError(\"Features must be selected first. Call fit().\")\n","        if not self.scaler:\n","            raise ValueError(\"Scaler must be fitted first. Call fit().\")\n","\n","        chunk_size = 100000\n","        num_chunks = int(np.ceil(len(test_data_raw) / chunk_size))\n","        all_predictions = []\n","        overlap = 60\n","\n","        # Ensure 'timestamp' is correctly identified in test_data_raw\n","        if 'timestamp' not in test_data_raw.columns:\n","            if test_data_raw.index.name == 'timestamp':\n","                test_data_raw.reset_index(inplace=True)\n","            elif 'index' in test_data_raw.columns:\n","                test_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n","            else:\n","                raise KeyError(\"The 'timestamp' column is missing from the test DataFrame and cannot be inferred.\")\n","\n","        # Store the original IDs before chunking, as predictions will be generated in order\n","        original_test_ids = test_data_raw['ID'].copy()\n","\n","        for i in range(num_chunks):\n","            print(f\"--> Processing chunk {i+1}/{num_chunks}...\")\n","\n","            start_idx = i * chunk_size\n","            end_idx = min((i + 1) * chunk_size, len(test_data_raw))\n","            start_with_overlap = max(0, start_idx - overlap) \n","            chunk = test_data_raw.iloc[start_with_overlap:end_idx].copy()\n","            \n","            chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n","\n","            base_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n","            cols_to_use_in_chunk = [col for col in base_cols + self.preselected_X_n_names if col in chunk.columns]\n","            \n","            if not cols_to_use_in_chunk:\n","                raise ValueError(\"No valid columns found in test chunk for feature engineering.\")\n","\n","            chunk = chunk[cols_to_use_in_chunk]\n","\n","            chunk_featured = self.create_time_features(chunk)\n","\n","            chunk_featured = chunk_featured.iloc[start_idx - start_with_overlap : ]\n","\n","            missing_features = [f for f in self.selected_features if f not in chunk_featured.columns]\n","            if missing_features:\n","                print(f\"Warning: Missing features in test chunk: {missing_features}. Filling with 0.\")\n","                for mf in missing_features:\n","                    chunk_featured[mf] = 0.0\n","\n","            X_test_df = chunk_featured[self.selected_features]\n","            X_test_scaled = self.scaler.transform(X_test_df)\n","            X_test_scaled = np.nan_to_num(X_test_scaled)\n","\n","            chunk_predictions = []\n","            for name, model in self.models.items():\n","                pred = model.predict(X_test_scaled)\n","                chunk_predictions.append(pred)\n","\n","            ensemble_prediction = np.mean(chunk_predictions, axis=0)\n","            all_predictions.append(ensemble_prediction)\n","\n","        final_predictions = np.concatenate(all_predictions)\n","        final_predictions = np.nan_to_num(final_predictions, nan=0.0)\n","\n","        predict_duration = time.time() - predict_start_time\n","        print(f\"Generated {len(final_predictions)} predictions in {predict_duration:.2f} seconds.\")\n","        return final_predictions, original_test_ids"]},{"cell_type":"code","execution_count":3,"id":"267a928f","metadata":{"execution":{"iopub.execute_input":"2025-07-17T08:00:39.092635Z","iopub.status.busy":"2025-07-17T08:00:39.092402Z","iopub.status.idle":"2025-07-17T08:02:24.732995Z","shell.execute_reply":"2025-07-17T08:02:24.732118Z"},"papermill":{"duration":105.644809,"end_time":"2025-07-17T08:02:24.734312","exception":false,"start_time":"2025-07-17T08:00:39.089503","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Warning: 'ID' column not found in train. Using DataFrame index as 'ID'. This might cause submission issues.\n","\n","Train shape: (525886, 788)\n","Test shape: (538150, 787)\n","🚀 Starting training pipeline...\n","Filtering data to the last 3 months for relevance and speed...\n","Training on data from 1969-10-01 to 1970-01-01. Shape: (525886, 788)\n","Found the top 25 'X' features: ['X19', 'X20', 'X21', 'X22', 'X28', 'X29', 'X219', 'X269', 'X270', 'X283', 'X285', 'X287', 'X289', 'X291', 'X293', 'X295', 'X466', 'X508', 'X614', 'X751', 'X752', 'X753', 'X754', 'X756', 'X759']\n","\n","--- 1. Use the intelligent Pre-selected Subset for Engineering ---\n","Using 31 columns to generate features\n","-----------------------------------------------------\n","\n","Memory usage before optimization: 124.38 MB\n","Memory usage after optimization: 72.22 MB\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (525886, 67)\n","\n","--- 2. All Features After Engineering Step ---\n","Created 67 total features: ['timestamp', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label', 'X19', 'X20', 'X21', 'X22', 'X28', 'X29', 'X219', 'X269', 'X270', 'X283', 'X285', 'X287', 'X289', 'X291', 'X293', 'X295', 'X466', 'X508', 'X614', 'X751', 'X752', 'X753', 'X754', 'X756', 'X759', 'mid_price', 'spread', 'imbalance', 'mid_price_ma_5', 'mid_price_std_5', 'mid_price_ma_10', 'mid_price_std_10', 'mid_price_ma_30', 'mid_price_std_30', 'mid_price_ma_60', 'mid_price_std_60', 'volume_ma_5', 'volume_std_5', 'volume_ma_10', 'volume_std_10', 'volume_ma_30', 'volume_std_30', 'volume_ma_60', 'volume_std_60', 'imbalance_ma_5', 'imbalance_std_5', 'imbalance_ma_10', 'imbalance_std_10', 'imbalance_ma_30', 'imbalance_std_30', 'imbalance_ma_60', 'imbalance_std_60', 'mid_price_lag_1', 'mid_price_lag_2', 'mid_price_lag_5', 'mid_price_lag_10', 'imbalance_lag_1', 'imbalance_lag_2', 'imbalance_lag_5', 'imbalance_lag_10', 'rsi_proxy_14']\n","------------------------------------------\n","\n","Selecting top 80 features from 65 features...\n","Selected 65 features.\n","Splitting data into training and validation sets based on time...\n","Warning: Time-based split resulted in too few training samples (0). Adjusting split to ensure minimum training size.\n","Train set size: 2, Validation set size: 525884\n","\n","Training LightGBM model...\n","Training XGBoost model...\n","Training CatBoost model...\n","\n","--- Validation Set Evaluation ---\n","📊 LGB - MAE: 0.8145, Pearson Correlation: nan\n","📊 XGB - MAE: 0.8147, Pearson Correlation: -0.0102\n","📊 CAT - MAE: 0.8061, Pearson Correlation: 0.0443\n","📊 ENSEMBLE - MAE: 0.8118, Pearson Correlation: 0.0402\n","\n","✅ Training pipeline complete.\n","\n","Generating predictions in chunks to manage memory...\n","--> Processing chunk 1/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100000, 66)\n","--> Processing chunk 2/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 66)\n","--> Processing chunk 3/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 66)\n","--> Processing chunk 4/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 66)\n","--> Processing chunk 5/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 66)\n","--> Processing chunk 6/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (38210, 66)\n","Generated 538150 predictions in 4.00 seconds.\n","\n","Performing final checks on generated predictions...\n","Predictions min: 0.5432, max: 0.5437\n","Number of NaNs after final check: 0\n","Number of Infs after final check: 0\n","\n","Submission file 'submission.csv' created successfully.\n","\n","First 5 rows of submission:\n","   ID  Prediction\n","0   1    0.543376\n","1   2    0.543376\n","2   3    0.543376\n","3   4    0.543376\n","4   5    0.543376\n","\n","Last 5 rows of submission:\n","            ID  Prediction\n","538145  538146    0.543376\n","538146  538147    0.543376\n","538147  538148    0.543376\n","538148  538149    0.543376\n","538149  538150    0.543376\n","\n","🧹 Cleaning up variables to free memory...\n","Cleanup complete. Ready for another run.\n"]}],"source":["# Cell 3 (Main function with the crucial ID handling fix and print statements)\n","def main():\n","    \"\"\"\n","    A wrapper function to define all variables locally and allow for cleanup.\n","    \"\"\"\n","    print(\"Loading data...\")\n","    train_full_raw = None\n","    test_full_raw = None\n","    predictor = None\n","    submission_df = None\n","\n","    try:\n","        # 1. LOAD DATA\n","        # Load train data\n","        train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n","        \n","        # Load test data\n","        test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n","        \n","        # --- NEW ROBUST ID/TIMESTAMP HANDLING ---\n","        def ensure_id_and_timestamp(df, df_name):\n","            df = df.reset_index()\n","\n","            timestamp_col = None\n","            for col in df.columns:\n","                try:\n","                    temp_series = pd.to_datetime(df[col], errors='coerce')\n","                    if not temp_series.isna().all() and col not in ['index', 'ID']:\n","                        timestamp_col = col\n","                        break\n","                except Exception:\n","                    pass\n","\n","            if timestamp_col and timestamp_col != 'timestamp':\n","                df.rename(columns={timestamp_col: 'timestamp'}, inplace=True)\n","            elif not timestamp_col and 'timestamp' not in df.columns:\n","                raise KeyError(f\"The 'timestamp' column is missing from the {df_name} DataFrame and cannot be inferred.\")\n","\n","            id_col = None\n","            if 'ID' in df.columns:\n","                id_col = 'ID'\n","            else:\n","                potential_id_cols = [col for col in df.columns if col not in ['timestamp'] and df[col].dtype in ['int64', 'int32', 'Int64']]\n","                \n","                level_id_col = None\n","                for p_id_col in potential_id_cols:\n","                    if p_id_col.startswith('level_'):\n","                        level_id_col = p_id_col\n","                        break\n","                \n","                if level_id_col:\n","                    id_col = level_id_col\n","                    df.rename(columns={id_col: 'ID'}, inplace=True)\n","                elif len(potential_id_cols) >= 1:\n","                    id_col = potential_id_cols[0]\n","                    df.rename(columns={id_col: 'ID'}, inplace=True)\n","                else:\n","                    df['ID'] = df.index\n","                    print(f\"Warning: 'ID' column not found in {df_name}. Using DataFrame index as 'ID'. This might cause submission issues.\")\n","\n","            df['timestamp'] = pd.to_datetime(df['timestamp'])\n","            \n","            if not pd.api.types.is_integer_dtype(df['ID']):\n","                try:\n","                    df['ID'] = pd.to_numeric(df['ID'], errors='coerce').astype('Int64') \n","                    if df['ID'].isnull().any():\n","                        print(f\"Warning: NaNs introduced in 'ID' column of {df_name} during conversion to integer. Filling with sequential numbers.\")\n","                        df.loc[df['ID'].isnull(), 'ID'] = np.arange(df['ID'].isnull().sum()) \n","                except Exception:\n","                    print(f\"Warning: Could not convert 'ID' column to integer in {df_name}. Keeping original type.\")\n","\n","            return df\n","\n","        train_full_raw = ensure_id_and_timestamp(train_full_raw, 'train')\n","        test_full_raw = ensure_id_and_timestamp(test_full_raw, 'test')\n","        # --- END NEW ROBUST ID/TIMESTAMP HANDLING ---\n","\n","\n","        print(f\"\\nTrain shape: {train_full_raw.shape}\")\n","        print(f\"Test shape: {test_full_raw.shape}\")\n","\n","        # 2. INITIALIZE AND TRAIN MODEL\n","        predictor = CryptoMarketPredictor(top_features=80, top_X_features_to_preselect=25)\n","        predictor.fit(train_full_raw)\n","\n","        # 3. GENERATE PREDICTIONS\n","        predictions, original_test_ids = predictor.predict(test_full_raw)\n","        \n","        # Final sanity checks on predictions before saving\n","        print(\"\\nPerforming final checks on generated predictions...\")\n","        predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n","        predictions = np.clip(predictions, -1.0, 1.0) # Adjust min/max if your target has a different range.\n","        print(f\"Predictions min: {np.min(predictions):.4f}, max: {np.max(predictions):.4f}\")\n","        print(f\"Number of NaNs after final check: {np.isnan(predictions).sum()}\")\n","        print(f\"Number of Infs after final check: {np.isinf(predictions).sum()}\")\n","\n","        # 4. CREATE SUBMISSION\n","        submission_df = pd.DataFrame({'ID': original_test_ids, 'Prediction': predictions})\n","        submission_df.to_csv('submission.csv', index=False)\n","        print(\"\\nSubmission file 'submission.csv' created successfully.\")\n","        \n","        # --- Print first 5 and last 5 rows of submission_df ---\n","        print(\"\\nFirst 5 rows of submission:\")\n","        print(submission_df.head())\n","        print(\"\\nLast 5 rows of submission:\")\n","        print(submission_df.tail())\n","        # --- END Print ---\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    finally:\n","        # 5. CLEANUP\n","        print(\"\\n🧹 Cleaning up variables to free memory...\")\n","        del train_full_raw\n","        del test_full_raw\n","        del predictor\n","        del submission_df\n","        gc.collect()\n","        print(\"Cleanup complete. Ready for another run.\")\n","\n","# --- This line calls the main function to start the pipeline ---\n","main()"]},{"cell_type":"code","execution_count":null,"id":"89be28e4","metadata":{"papermill":{"duration":0.003103,"end_time":"2025-07-17T08:02:24.741007","exception":false,"start_time":"2025-07-17T08:02:24.737904","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":12993472,"sourceId":96164,"sourceType":"competition"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":129.940841,"end_time":"2025-07-17T08:02:26.366743","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-17T08:00:16.425902","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}