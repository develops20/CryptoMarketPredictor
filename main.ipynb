{"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-cb-lgbm-xgb-nb153?scriptVersionId=251045027\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","execution_count":1,"id":"85e44e80","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2025-07-17T15:28:39.991281Z","iopub.status.busy":"2025-07-17T15:28:39.990792Z","iopub.status.idle":"2025-07-17T15:28:57.485811Z","shell.execute_reply":"2025-07-17T15:28:57.484945Z"},"papermill":{"duration":17.500456,"end_time":"2025-07-17T15:28:57.487401","exception":false,"start_time":"2025-07-17T15:28:39.986945","status":"completed"},"tags":[]},"outputs":[],"source":["# Step 1: Installations\n","!pip install catboost lightgbm xgboost prophet -q # Added prophet\n","\n","# Step 2: Imports\n","import pandas as pd\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore')\n","import os\n","import gc\n","from sklearn.preprocessing import RobustScaler\n","from sklearn.feature_selection import SelectKBest, f_regression\n","from sklearn.metrics import mean_absolute_error\n","from scipy.stats import pearsonr\n","import lightgbm as lgb\n","import xgboost as xgb\n","import catboost as cb\n","import time\n","from prophet import Prophet # Added Prophet import"]},{"cell_type":"code","execution_count":2,"id":"528d2f97","metadata":{"execution":{"iopub.execute_input":"2025-07-17T15:28:57.494126Z","iopub.status.busy":"2025-07-17T15:28:57.493602Z","iopub.status.idle":"2025-07-17T15:28:57.643889Z","shell.execute_reply":"2025-07-17T15:28:57.643081Z"},"papermill":{"duration":0.154989,"end_time":"2025-07-17T15:28:57.645096","exception":false,"start_time":"2025-07-17T15:28:57.490107","status":"completed"},"tags":[]},"outputs":[],"source":["# Step 3: Class Definition (No changes needed here. The problem is in data loading/ID handling.)\n","class CryptoMarketPredictor:\n","    \"\"\"\n","    An optimized pipeline using an ensemble of tree-based models (LGBM, XGBoost, CatBoost)\n","    with rich feature engineering to predict crypto market movements.\n","    \"\"\"\n","    def __init__(self, top_features=100, top_X_features_to_preselect=30, use_future_lags=False):\n","        self.top_features = top_features\n","        self.top_X_features_to_preselect = top_X_features_to_preselect\n","        self.scaler = RobustScaler()\n","        self.feature_selector = None\n","        self.selected_features = None\n","        self.models = {}\n","        self.preselected_X_n_names = None\n","        self.use_future_lags = use_future_lags # New parameter\n","\n","    def optimize_memory(self, df):\n","        \"\"\"Optimize DataFrame memory usage.\"\"\"\n","        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","        for col in df.columns:\n","            col_type = df[col].dtype\n","            if col_type != 'object' and col not in ['timestamp', 'ID', 'label']: # Exclude timestamp, ID, label from downcasting\n","                c_min = df[col].min()\n","                c_max = df[col].max()\n","                if str(col_type)[:3] == 'int':\n","                    if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n","                        df[col] = df[col].astype(np.int8)\n","                    elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n","                        df[col] = df[col].astype(np.int16)\n","                    elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n","                        df[col] = df[col].astype(np.int32)\n","                    elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n","                        df[col] = df[col].astype(np.int64)\n","                else: # float\n","                    if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n","                        df[col] = df[col].astype(np.float32)\n","        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n","        return df\n","\n","    def clean_data(self, df):\n","        \"\"\"Clean DataFrame by handling inf, NaN values.\"\"\"\n","        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        # Use forward-fill then backward-fill to propagate last valid observation\n","        for col in df.columns:\n","            if df[col].dtype == 'float32' or df[col].dtype == 'float64':\n","                df[col] = df[col].ffill().bfill()\n","        df.fillna(0, inplace=True) # Fill any remaining NaNs with 0\n","        return df\n","\n","    def calculate_rsi_proxy(self, prices_series, window=14):\n","        \"\"\"Calculate a proxy for the Relative Strength Index (RSI).\"\"\"\n","        delta = prices_series.diff()\n","        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n","        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n","        rs = gain / (loss + 1e-10) # Add epsilon to avoid division by zero\n","        rsi = 100 - (100 / (1 + rs))\n","        return rsi.fillna(50) # Fill initial NaNs with 50 (neutral RSI)\n","\n","    def create_time_features(self, df):\n","        \"\"\"Engineer a rich set of time-based features to capture market memory.\"\"\"\n","        print(\"🛠️ Engineering rich time-based features...\")\n","\n","        # Basic price and imbalance features\n","        df['mid_price'] = (df['ask_qty'] + df['bid_qty']) / 2 if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n","        df['spread'] = df['ask_qty'] - df['bid_qty'] if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n","        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1e-10) \\\n","                          if 'bid_qty' in df.columns and 'ask_qty' in df.columns else 0.0\n","\n","        # --- NEW CUSTOM FEATURES from the 0.9 notebook ---\n","        if 'bid_qty' in df.columns and 'ask_qty' in df.columns:\n","            df['bid_ask_ratio'] = df['bid_qty'] / (df['ask_qty'] + 1e-10)\n","        else:\n","            df['bid_ask_ratio'] = 0.0\n","\n","        if 'buy_qty' in df.columns and 'sell_qty' in df.columns:\n","            df['buy_sell_ratio'] = df['buy_qty'] / (df['sell_qty'] + 1e-10)\n","        else:\n","            df['buy_sell_ratio'] = 0.0\n","        # --- END NEW CUSTOM FEATURES ---\n","        \n","        # Ensure 'volume' column exists before creating rolling features\n","        cols_for_rolling = ['mid_price', 'volume', 'imbalance']\n","        cols_for_rolling = [col for col in cols_for_rolling if col in df.columns]\n","\n","        # Rolling window features (MA and STD)\n","        rolling_windows = [5, 10, 30, 60]\n","        for col in cols_for_rolling:\n","            for window in rolling_windows:\n","                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n","                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n","\n","        # Lag features - adjusted based on use_future_lags\n","        lag_periods = [1, 2, 5, 10]\n","        cols_for_lags = ['mid_price', 'imbalance']\n","        cols_for_lags = [col for col in cols_for_lags if col in df.columns]\n","        for col in cols_for_lags:\n","            for lag in lag_periods:\n","                if self.use_future_lags:\n","                    df[f'{col}_lag_{lag}'] = df[col].shift(-lag) # Future lags (lead values)\n","                else:\n","                    df[f'{col}_lag_{lag}'] = df[col].shift(lag) # Past lags\n","\n","        # Momentum feature - ensure 'mid_price' exists\n","        if 'mid_price' in df.columns:\n","            df['rsi_proxy_14'] = self.calculate_rsi_proxy(df['mid_price'], window=14)\n","        else:\n","            df['rsi_proxy_14'] = 50.0 # Default if mid_price is not available\n","\n","        # Final cleanup to handle NaNs introduced by rolling/shifting\n","        df = self.clean_data(df)\n","        print(f\"Feature engineering complete. Shape: {df.shape}\")\n","        return df\n","\n","\n","    def select_features(self, X_df, y_df):\n","        \"\"\"Select top k features using f_regression to reduce dimensionality.\"\"\"\n","        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n","        n_features_to_select = min(self.top_features, X_df.shape[1])\n","        selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n","        \n","        X_df_clean = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n","        y_df_clean = y_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n","\n","        if y_df_clean.nunique() == 1:\n","            print(\"Warning: Target variable is constant. Skipping feature selection.\")\n","            self.selected_features = X_df_clean.columns.tolist()\n","            self.feature_selector = None\n","            return X_df_clean.values\n","            \n","        selector.fit(X_df_clean, y_df_clean)\n","        self.feature_selector = selector\n","        self.selected_features = X_df_clean.columns[selector.get_support()].tolist()\n","        print(f\"Selected {len(self.selected_features)} features.\")\n","        return X_df_clean[self.selected_features].values\n","\n","    def evaluate_model(self, y_true, y_pred, model_name):\n","        \"\"\"Evaluate model performance and print results.\"\"\"\n","        mae = mean_absolute_error(y_true, y_pred)\n","        \n","        if np.std(y_true) == 0 or np.std(y_pred) == 0:\n","            correlation = np.nan\n","        else:\n","            correlation, _ = pearsonr(y_true, y_pred)\n","        \n","        print(f\"📊 {model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n","        return correlation\n","\n","    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train a LightGBM model.\"\"\"\n","        print(\"\\nTraining LightGBM model...\")\n","        params = {\n","            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1500,\n","            'learning_rate': 0.03, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n","            'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1,\n","            'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42\n","        }\n","        model = lgb.LGBMRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n","                  callbacks=[lgb.early_stopping(100, verbose=False)])\n","        return model\n","\n","    def train_xgboost(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train an XGBoost model.\"\"\"\n","        print(\"Training XGBoost model...\")\n","        params = {\n","            'objective': 'reg:squarederror',\n","            'eval_metric': 'mae',\n","            'n_estimators': 1500,\n","            'learning_rate': 0.03,\n","            'tree_method': 'hist',\n","            'subsample': 0.8,\n","            'colsample_bytree': 0.8,\n","            'seed': 42,\n","            'n_jobs': -1,\n","        }\n","        model = xgb.XGBRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n","                  early_stopping_rounds=100, verbose=False) \n","        return model\n","\n","    def train_catboost(self, X_train, y_train, X_val, y_val):\n","        \"\"\"Train a CatBoost model.\"\"\"\n","        print(\"Training CatBoost model...\")\n","        params = {\n","            'objective': 'MAE', 'eval_metric': 'MAE', 'iterations': 1500,\n","            'learning_rate': 0.03, 'random_seed': 42, 'logging_level': 'Silent',\n","            'l2_leaf_reg': 3, 'bagging_temperature': 1,\n","        }\n","        model = cb.CatBoostRegressor(**params)\n","        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n","        return model\n","\n","    # ---------------------------------FIT METHOD----------------------------------------------------\n","    def fit(self, train_data_raw):\n","        \"\"\"Main training pipeline.\"\"\"\n","        fit_start_time = time.time()\n","        print(\"🚀 Starting training pipeline...\")\n","\n","        # Ensure 'timestamp' exists and is datetime\n","        if 'timestamp' not in train_data_raw.columns:\n","            if train_data_raw.index.name == 'timestamp':\n","                train_data_raw.reset_index(inplace=True)\n","            elif 'index' in train_data_raw.columns:\n","                 train_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n","            else:\n","                raise KeyError(\"The 'timestamp' column is missing from the DataFrame and cannot be inferred.\")\n","        \n","        train_data_raw['timestamp'] = pd.to_datetime(train_data_raw['timestamp'])\n","\n","        # --- 1. Filter data to the most recent 3 months ---\n","        print(\"Filtering data to the last 3 months for relevance and speed...\")\n","        max_date_full = train_data_raw['timestamp'].max()\n","        three_months_prior = max_date_full - pd.DateOffset(months=3)\n","        train_df_filtered = train_data_raw[train_data_raw['timestamp'] >= three_months_prior].copy()\n","        \n","        if train_df_filtered.empty:\n","            raise ValueError(\"Training DataFrame is empty after filtering to the last 3 months. Adjust filter or provide more data.\")\n","\n","        print(f\"Training on data from {train_df_filtered['timestamp'].min().date()} to {train_df_filtered['timestamp'].max().date()}. Shape: {train_df_filtered.shape}\")\n","        \n","        # Ensure train_df is sorted by timestamp after filtering\n","        print(\"Sorting train_df by timestamp for chronological splitting...\")\n","        train_df_filtered.sort_values(by='timestamp', inplace=True)\n","        train_df_filtered.reset_index(drop=True, inplace=True) # Reset index after sort for clean slicing\n","\n","        # Now, use this filtered and sorted DataFrame for all subsequent steps\n","        train_df = train_df_filtered \n","        del train_data_raw, train_df_filtered; gc.collect() # Free up memory from original and temp filtered df\n","\n","        # --- 1a. More intelligent preselection of the anonymous features ---\n","        sample_df = train_df.sample(n=min(50000, len(train_df)), random_state=42)\n","        X_n_cols_raw = [c for c in sample_df.columns if c.startswith('X')]\n","        X_raw_sample = sample_df[X_n_cols_raw].copy()\n","        X_raw_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n","        X_raw_sample.fillna(0, inplace=True)\n","        y_raw_sample = sample_df['label'].fillna(0)\n","\n","        pre_selector = SelectKBest(score_func=f_regression, k=self.top_X_features_to_preselect)\n","        if y_raw_sample.nunique() == 1:\n","            print(\"Warning: Sample target variable is constant. Preselecting all 'X' features.\")\n","            self.preselected_X_n_names = X_raw_sample.columns.tolist()\n","        else:\n","            pre_selector.fit(X_raw_sample, y_raw_sample)\n","            self.preselected_X_n_names = X_raw_sample.columns[pre_selector.get_support()].tolist()\n","\n","        print(f\"Found the top {len(self.preselected_X_n_names)} 'X' features: {self.preselected_X_n_names}\")\n","        del sample_df, X_raw_sample, y_raw_sample, pre_selector; gc.collect()\n","\n","        # --- 2. Preprocessing and Feature Engineering ---\n","        # Add new custom features to the list of columns to use\n","        base_cols_for_fe = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label']\n","        cols_to_use = base_cols_for_fe + self.preselected_X_n_names\n","        cols_to_use = [col for col in cols_to_use if col in train_df.columns]\n","\n","        print(\"\\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\")\n","        print(f\"Using {len(cols_to_use)} columns to generate features\")\n","        print(\"-----------------------------------------------------\\n\")\n","\n","        train_df = train_df[cols_to_use]\n","        train_df = self.optimize_memory(train_df)\n","        train_df = self.create_time_features(train_df) # This will now create new features like bid_ask_ratio\n","\n","        print(\"\\n--- 2. All Features After Engineering Step ---\")\n","        engineered_features_list = train_df.columns.tolist()\n","        print(f\"Created {len(engineered_features_list)} total features: {engineered_features_list}\")\n","        print(\"------------------------------------------\\n\")\n","\n","        # --- 3. Feature Selection ---\n","        feature_cols = [c for c in train_df.columns if c not in ['timestamp', 'ID', 'label']] # Exclude ID here too\n","        X_df = train_df[feature_cols]\n","        y_df_aligned = train_df['label'] # Align y_df with the current train_df after all processing\n","\n","        X_selected_array = self.select_features(X_df, y_df_aligned)\n","        \n","        # --- 4. Scaling and Time-Based Validation Split (using last N rows) ---\n","        print(\"Splitting data into training and validation sets using last N rows for validation...\")\n","        X_scaled = self.scaler.fit_transform(X_selected_array)\n","        \n","        total_samples = len(X_scaled)\n","        min_samples_required = 2 \n","        val_percentage = 0.2 # Use 20% of data for validation\n","\n","        val_size = max(min_samples_required, int(total_samples * val_percentage))\n","        train_size = total_samples - val_size\n","\n","        if train_size < min_samples_required:\n","            raise ValueError(f\"Not enough data to create valid training set ({train_size} samples) after reserving {val_size} for validation. Total samples: {total_samples}\")\n","        if val_size < min_samples_required:\n","            raise ValueError(f\"Not enough data to create valid validation set ({val_size} samples). Total samples: {total_samples}\")\n","\n","        X_train, X_val = X_scaled[:train_size], X_scaled[train_size:]\n","        y_train, y_val = y_df_aligned.iloc[:train_size], y_df_aligned.iloc[train_size:]\n","\n","        print(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")\n","        del X_df, y_df_aligned, X_selected_array, X_scaled, train_df; gc.collect()\n","\n","        # --- 5. Train Models ---\n","        if X_train.shape[0] < min_samples_required or X_val.shape[0] < min_samples_required:\n","            raise ValueError(f\"Train or Validation set has fewer than {min_samples_required} samples after splitting. Cannot train models.\")\n","\n","        self.models['lgb'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n","        self.models['xgb'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n","        self.models['cat'] = self.train_catboost(X_train, y_train, X_val, y_val)\n","\n","        # --- 6. Evaluate Ensemble on Validation Set ---\n","        print(\"\\n--- Validation Set Evaluation ---\")\n","        val_predictions = {}\n","        for name, model in self.models.items():\n","            val_predictions[name] = model.predict(X_val)\n","            self.evaluate_model(y_val, val_predictions[name], name.upper())\n","\n","        ensemble_pred = np.mean([val_predictions['lgb'], val_predictions['xgb'], val_predictions['cat']], axis=0)\n","        self.evaluate_model(y_val, ensemble_pred, \"ENSEMBLE\")\n","\n","        print(\"\\n✅ Training pipeline complete.\")\n","        return self\n","\n","    def predict(self, test_data_raw):\n","        \"\"\"\n","        Generate predictions for test data in chunks to manage memory.\n","        \"\"\"\n","        predict_start_time = time.time()\n","        print(\"\\nGenerating predictions in chunks to manage memory...\")\n","        if not self.models:\n","            raise ValueError(\"Models must be trained first. Call fit().\")\n","        if not self.selected_features:\n","             raise ValueError(\"Features must be selected first. Call fit().\")\n","        if not self.scaler:\n","            raise ValueError(\"Scaler must be fitted first. Call fit().\")\n","\n","        chunk_size = 100000\n","        num_chunks = int(np.ceil(len(test_data_raw) / chunk_size))\n","        all_predictions = []\n","        overlap = 60\n","\n","        # Ensure 'timestamp' is correctly identified in test_data_raw\n","        if 'timestamp' not in test_data_raw.columns:\n","            if test_data_raw.index.name == 'timestamp':\n","                test_data_raw.reset_index(inplace=True)\n","            elif 'index' in test_data_raw.columns:\n","                test_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n","            else:\n","                raise KeyError(\"The 'timestamp' column is missing from the test DataFrame and cannot be inferred.\")\n","\n","        # Store the original IDs before chunking, as predictions will be generated in order\n","        original_test_ids = test_data_raw['ID'].copy()\n","\n","        for i in range(num_chunks):\n","            print(f\"--> Processing chunk {i+1}/{num_chunks}...\")\n","\n","            start_idx = i * chunk_size\n","            end_idx = min((i + 1) * chunk_size, len(test_data_raw))\n","            start_with_overlap = max(0, start_idx - overlap) \n","            chunk = test_data_raw.iloc[start_with_overlap:end_idx].copy()\n","            \n","            chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n","\n","            # Include new custom features in base_cols\n","            base_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n","            cols_to_use_in_chunk = [col for col in base_cols + self.preselected_X_n_names if col in chunk.columns]\n","            \n","            if not cols_to_use_in_chunk:\n","                raise ValueError(\"No valid columns found in test chunk for feature engineering.\")\n","\n","            chunk = chunk[cols_to_use_in_chunk]\n","\n","            chunk_featured = self.create_time_features(chunk) # This will now create new features like bid_ask_ratio\n","\n","            chunk_featured = chunk_featured.iloc[start_idx - start_with_overlap : ]\n","\n","            missing_features = [f for f in self.selected_features if f not in chunk_featured.columns]\n","            if missing_features:\n","                print(f\"Warning: Missing features in test chunk: {missing_features}. Filling with 0.\")\n","                for mf in missing_features:\n","                    chunk_featured[mf] = 0.0\n","\n","            X_test_df = chunk_featured[self.selected_features]\n","            X_test_scaled = self.scaler.transform(X_test_df)\n","            X_test_scaled = np.nan_to_num(X_test_scaled)\n","\n","            chunk_predictions = []\n","            for name, model in self.models.items():\n","                pred = model.predict(X_test_scaled)\n","                chunk_predictions.append(pred)\n","\n","            ensemble_prediction = np.mean(chunk_predictions, axis=0)\n","            all_predictions.append(ensemble_prediction)\n","\n","        final_predictions = np.concatenate(all_predictions)\n","        final_predictions = np.nan_to_num(final_predictions, nan=0.0)\n","\n","        predict_duration = time.time() - predict_start_time\n","        print(f\"Generated {len(final_predictions)} predictions in {predict_duration:.2f} seconds.\")\n","        return final_predictions, original_test_ids\n","\n","# --- NEW FUNCTION FOR PROPHET ENHANCEMENT ---\n","def train_prophet_enhancement(df_ensemble, prophet_weight=0.085):\n","    \"\"\"\n","    Train a Prophet model on the ensemble predictions and blend with small weight.\n","    \n","    Parameters:\n","    df_ensemble: DataFrame with 'ID' and 'Prediction' columns from the initial ensemble.\n","    prophet_weight: Weight for Prophet predictions in the final blend.\n","    \"\"\"\n","    print(\"\\n--- Starting Prophet Enhancement ---\")\n","    print(\"Training Prophet model for ensemble enhancement...\")\n","    \n","    # Prepare data for Prophet: 'ds' (datetime) and 'y' (value)\n","    prophet_df = pd.DataFrame()\n","    # Convert IDs to hours from a base date to represent time for Prophet\n","    # Assuming IDs are sequential and represent time steps\n","    base_date = pd.to_datetime('2024-01-01') # Use a recent, arbitrary base date\n","    prophet_df['ds'] = base_date + pd.to_timedelta(df_ensemble['ID'] - df_ensemble['ID'].min(), unit='H')\n","    prophet_df['y'] = df_ensemble['Prediction'] # Use the ensemble's prediction as Prophet's target\n","\n","    # Prophet model configuration (tuned from the 0.9 notebook)\n","    model = Prophet(\n","        growth='linear',\n","        changepoint_prior_scale=0.06,\n","        changepoint_range=0.9,\n","        n_changepoints=75,\n","        yearly_seasonality=True,\n","        weekly_seasonality=True,\n","        daily_seasonality=False, # Assuming data points are hourly, daily seasonality might be too fine-grained or not present\n","        seasonality_mode='multiplicative',\n","        seasonality_prior_scale=10.0,\n","        holidays_prior_scale=10.0,\n","        interval_width=0.95,\n","        uncertainty_samples=0, # Set to 0 for faster prediction, 300 for full Bayesian inference (slower)\n","    )\n","    \n","    # Add custom seasonalities for potential patterns (based on hourly IDs)\n","    # 730 hours ~ 30 days (monthly pattern)\n","    model.add_seasonality(name='pattern_730h', period=730, fourier_order=5, prior_scale=10)\n","    # 2190 hours ~ 3 months (quarterly pattern)\n","    model.add_seasonality(name='pattern_2190h', period=2190, fourier_order=10, prior_scale=5)\n","    \n","    # Add 'abs(mx-m)' as a regressor if you decide to calculate it in your pipeline\n","    # For now, we'll skip it as your current pipeline doesn't produce it.\n","    # If you want to add it, you'd need to modify your CryptoMarketPredictor to output it\n","    # and then pass it to this function via df_ensemble.\n","    # Example: if 'abs(mx-m)' in df_ensemble.columns:\n","    #     prophet_df['abs_mx_m'] = df_ensemble['abs(mx-m)']\n","    #     model.add_regressor('abs_mx_m', prior_scale=5.0)\n","\n","    print(\"Fitting Prophet model...\")\n","    model.fit(prophet_df)\n","    \n","    # Make predictions for the future (which is just the existing IDs)\n","    future = prophet_df[['ds']].copy()\n","    # If you added regressors, you'd need to add them to 'future' too:\n","    # if 'abs_mx_m' in prophet_df.columns:\n","    #    future['abs_mx_m'] = prophet_df['abs_mx_m']\n","\n","    forecast = model.predict(future)\n","    \n","    prophet_predictions = forecast['yhat'].values\n","    \n","    # Blend with initial ensemble predictions\n","    ensemble_predictions = df_ensemble['Prediction'].values # Note: 'Prediction' column from your ensemble\n","    final_predictions = (1 - prophet_weight) * ensemble_predictions + prophet_weight * prophet_predictions\n","    \n","    df_final_prophet = pd.DataFrame({\n","        'ID': df_ensemble['ID'],\n","        'Prediction': final_predictions\n","    })\n","    \n","    print(f\"\\nProphet Enhancement Statistics:\")\n","    print(f\"Prophet weight used: {prophet_weight}\")\n","    print(f\"Mean adjustment (Prophet_pred - Ensemble_pred): {np.mean(prophet_predictions - ensemble_predictions):.6f}\")\n","    print(f\"Std adjustment: {np.std(prophet_predictions - ensemble_predictions):.6f}\")\n","    print(f\"Max absolute adjustment: {np.max(np.abs(prophet_predictions - ensemble_predictions)):.6f}\")\n","    print(\"--- Prophet Enhancement Complete ---\")\n","    \n","    return df_final_prophet\n"]},{"cell_type":"code","execution_count":3,"id":"b0a844fc","metadata":{"execution":{"iopub.execute_input":"2025-07-17T15:28:57.650613Z","iopub.status.busy":"2025-07-17T15:28:57.650241Z","iopub.status.idle":"2025-07-17T15:41:47.583879Z","shell.execute_reply":"2025-07-17T15:41:47.582983Z"},"papermill":{"duration":769.937846,"end_time":"2025-07-17T15:41:47.585218","exception":false,"start_time":"2025-07-17T15:28:57.647372","status":"completed"},"tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading data...\n","Warning: 'ID' column not found in train. Using DataFrame index as 'ID'. This might cause submission issues.\n","\n","Train shape: (525886, 788)\n","Test shape: (538150, 787)\n","No timestamp reconstruction file found.\n","🚀 Starting training pipeline...\n","Filtering data to the last 3 months for relevance and speed...\n","Training on data from 1970-01-01 to 1970-01-01. Shape: (525886, 788)\n","Sorting train_df by timestamp for chronological splitting...\n","Found the top 25 'X' features: ['X19', 'X20', 'X21', 'X22', 'X27', 'X28', 'X29', 'X218', 'X219', 'X287', 'X289', 'X291', 'X293', 'X295', 'X297', 'X298', 'X299', 'X465', 'X466', 'X614', 'X744', 'X751', 'X752', 'X753', 'X759']\n","\n","--- 1. Use the intelligent Pre-selected Subset for Engineering ---\n","Using 31 columns to generate features\n","-----------------------------------------------------\n","\n","Memory usage before optimization: 124.38 MB\n","Memory usage after optimization: 66.20 MB\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (525886, 69)\n","\n","--- 2. All Features After Engineering Step ---\n","Created 69 total features: ['timestamp', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label', 'X19', 'X20', 'X21', 'X22', 'X27', 'X28', 'X29', 'X218', 'X219', 'X287', 'X289', 'X291', 'X293', 'X295', 'X297', 'X298', 'X299', 'X465', 'X466', 'X614', 'X744', 'X751', 'X752', 'X753', 'X759', 'mid_price', 'spread', 'imbalance', 'bid_ask_ratio', 'buy_sell_ratio', 'mid_price_ma_5', 'mid_price_std_5', 'mid_price_ma_10', 'mid_price_std_10', 'mid_price_ma_30', 'mid_price_std_30', 'mid_price_ma_60', 'mid_price_std_60', 'volume_ma_5', 'volume_std_5', 'volume_ma_10', 'volume_std_10', 'volume_ma_30', 'volume_std_30', 'volume_ma_60', 'volume_std_60', 'imbalance_ma_5', 'imbalance_std_5', 'imbalance_ma_10', 'imbalance_std_10', 'imbalance_ma_30', 'imbalance_std_30', 'imbalance_ma_60', 'imbalance_std_60', 'mid_price_lag_1', 'mid_price_lag_2', 'mid_price_lag_5', 'mid_price_lag_10', 'imbalance_lag_1', 'imbalance_lag_2', 'imbalance_lag_5', 'imbalance_lag_10', 'rsi_proxy_14']\n","------------------------------------------\n","\n","Selecting top 80 features from 67 features...\n","Selected 67 features.\n","Splitting data into training and validation sets using last N rows for validation...\n","Train set size: 420709, Validation set size: 105177\n","\n","Training LightGBM model...\n","Training XGBoost model...\n","Training CatBoost model...\n","\n","--- Validation Set Evaluation ---\n","📊 LGB - MAE: 0.4404, Pearson Correlation: 0.5999\n","📊 XGB - MAE: 0.3998, Pearson Correlation: 0.7840\n","📊 CAT - MAE: 0.4788, Pearson Correlation: 0.5117\n","📊 ENSEMBLE - MAE: 0.4351, Pearson Correlation: 0.7163\n","\n","✅ Training pipeline complete.\n","\n","Generating predictions in chunks to manage memory...\n","--> Processing chunk 1/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100000, 68)\n","--> Processing chunk 2/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 68)\n","--> Processing chunk 3/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 68)\n","--> Processing chunk 4/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 68)\n","--> Processing chunk 5/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (100060, 68)\n","--> Processing chunk 6/6...\n","🛠️ Engineering rich time-based features...\n","Feature engineering complete. Shape: (38210, 68)\n","Generated 538150 predictions in 33.79 seconds.\n","\n","Performing final checks on generated predictions...\n","Predictions min: -1.0000, max: 1.0000\n","Number of NaNs after final check: 0\n","Number of Infs after final check: 0\n","\n","--- Starting Prophet Enhancement ---\n","Training Prophet model for ensemble enhancement...\n","Fitting Prophet model...\n"]},{"name":"stderr","output_type":"stream","text":["15:35:28 - cmdstanpy - INFO - Chain [1] start processing\n","15:41:43 - cmdstanpy - INFO - Chain [1] done processing\n"]},{"name":"stdout","output_type":"stream","text":["\n","Prophet Enhancement Statistics:\n","Prophet weight used: 0.085\n","Mean adjustment (Prophet_pred - Ensemble_pred): -0.000131\n","Std adjustment: 0.387907\n","Max absolute adjustment: 1.003302\n","--- Prophet Enhancement Complete ---\n","\n","Submission file 'submission.csv' created successfully.\n","\n","First 5 rows of submission:\n","   ID  Prediction\n","0   1    0.140888\n","1   2   -0.353134\n","2   3   -0.421817\n","3   4   -0.116849\n","4   5   -0.647758\n","\n","Last 5 rows of submission:\n","            ID  Prediction\n","538145  538146   -0.268505\n","538146  538147   -0.353183\n","538147  538148   -0.597914\n","538148  538149   -0.140501\n","538149  538150    0.038379\n","\n","🧹 Cleaning up variables to free memory...\n","Cleanup complete. Ready for another run.\n"]}],"source":["# Cell 3 (Main function with the crucial ID handling fix and print statements)\n","def main():\n","    \"\"\"\n","    A wrapper function to define all variables locally and allow for cleanup.\n","    \"\"\"\n","    print(\"Loading data...\")\n","    train_full_raw = None\n","    test_full_raw = None\n","    predictor = None\n","    # Initialize all variables that will be 'del'eted in finally block\n","    initial_submission_df = None\n","    final_submission_df = None\n","    sample_submission = None \n","\n","    try:\n","        # 1. LOAD DATA\n","        train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n","        test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n","        \n","        # --- ROBUST ID/TIMESTAMP HANDLING ---\n","        def ensure_id_and_timestamp(df, df_name):\n","            df = df.reset_index()\n","\n","            timestamp_col = None\n","            for col in df.columns:\n","                try:\n","                    temp_series = pd.to_datetime(df[col], errors='coerce')\n","                    if not temp_series.isna().all() and col not in ['index', 'ID']:\n","                        timestamp_col = col\n","                        break\n","                except Exception:\n","                    pass\n","\n","            if timestamp_col and timestamp_col != 'timestamp':\n","                df.rename(columns={timestamp_col: 'timestamp'}, inplace=True)\n","            elif not timestamp_col and 'timestamp' not in df.columns:\n","                raise KeyError(f\"The 'timestamp' column is missing from the {df_name} DataFrame and cannot be inferred.\")\n","\n","            id_col = None\n","            if 'ID' in df.columns:\n","                id_col = 'ID'\n","            else:\n","                potential_id_cols = [col for col in df.columns if col not in ['timestamp'] and df[col].dtype in ['int64', 'int32', 'Int64']]\n","                \n","                level_id_col = None\n","                for p_id_col in potential_id_cols:\n","                    if p_id_col.startswith('level_'):\n","                        level_id_col = p_id_col\n","                        break\n","                \n","                if level_id_col:\n","                    id_col = level_id_col\n","                    df.rename(columns={id_col: 'ID'}, inplace=True)\n","                elif len(potential_id_cols) >= 1:\n","                    id_col = potential_id_cols[0]\n","                    df.rename(columns={id_col: 'ID'}, inplace=True)\n","                else:\n","                    df['ID'] = df.index\n","                    print(f\"Warning: 'ID' column not found in {df_name}. Using DataFrame index as 'ID'. This might cause submission issues.\")\n","\n","            df['timestamp'] = pd.to_datetime(df['timestamp'])\n","            \n","            if not pd.api.types.is_integer_dtype(df['ID']):\n","                try:\n","                    df['ID'] = pd.to_numeric(df['ID'], errors='coerce').astype('Int64') \n","                    if df['ID'].isnull().any():\n","                        print(f\"Warning: NaNs introduced in 'ID' column of {df_name} during conversion to integer. Filling with sequential numbers.\")\n","                        df.loc[df['ID'].isnull(), 'ID'] = np.arange(df['ID'].isnull().sum()) \n","                except Exception:\n","                    print(f\"Warning: Could not convert 'ID' column to integer in {df_name}. Keeping original type.\")\n","\n","            return df\n","\n","        train_full_raw = ensure_id_and_timestamp(train_full_raw, 'train')\n","        test_full_raw = ensure_id_and_timestamp(test_full_raw, 'test')\n","        # --- END NEW ROBUST ID/TIMESTAMP HANDLING ---\n","\n","\n","        print(f\"\\nTrain shape: {train_full_raw.shape}\")\n","        print(f\"Test shape: {test_full_raw.shape}\")\n","\n","        # --- NEW: Timestamp Reconstruction for Test Data ---\n","        timestamp_recon_path = '/kaggle/input/the-order-of-the-test-rows-2/closest_rows.csv'\n","        use_timestamp_reconstruction = os.path.exists(timestamp_recon_path)\n","\n","        if use_timestamp_reconstruction:\n","            print(\"\\nApplying timestamp reconstruction for test data...\")\n","            try:\n","                t_recon = pd.read_csv(timestamp_recon_path, header=None).iloc[:, 0]\n","                new_order_indices = t_recon.to_numpy()\n","                \n","                test_full_raw.reset_index(drop=True, inplace=True)\n","                \n","                test_full_raw = test_full_raw.iloc[new_order_indices].copy()\n","                test_full_raw.reset_index(drop=True, inplace=True)\n","                print(\"Test data sorted by reconstructed timestamps.\")\n","\n","            except Exception as e:\n","                print(f\"Warning: Failed to apply timestamp reconstruction: {e}. Proceeding without it.\")\n","                traceback.print_exc()\n","                use_timestamp_reconstruction = False\n","        else:\n","            print(\"No timestamp reconstruction file found.\")\n","        # --- END NEW: Timestamp Reconstruction ---\n","\n","\n","        # 2. INITIALIZE AND TRAIN MODEL\n","        predictor = CryptoMarketPredictor(top_features=80, top_X_features_to_preselect=25, use_future_lags=False)\n","        predictor.fit(train_full_raw)\n","\n","        # 3. GENERATE PREDICTIONS\n","        predictions, original_test_ids = predictor.predict(test_full_raw)\n","        \n","        # Final sanity checks on predictions before saving\n","        print(\"\\nPerforming final checks on generated predictions...\")\n","        predictions = np.nan_to_num(predictions, nan=0.0, posinf=1.0, neginf=-1.0)\n","        predictions = np.clip(predictions, -1.0, 1.0)\n","        print(f\"Predictions min: {np.min(predictions):.4f}, max: {np.max(predictions):.4f}\")\n","        print(f\"Number of NaNs after final check: {np.isnan(predictions).sum()}\")\n","        print(f\"Number of Infs after final check: {np.isinf(predictions).sum()}\")\n","\n","        # 4. CREATE INITIAL SUBMISSION DF (before Prophet)\n","        initial_submission_df = pd.DataFrame({'ID': original_test_ids, 'Prediction': predictions})\n","        \n","        # --- NEW: Apply Prophet Enhancement ---\n","        prophet_weight_to_use = 0.085\n","        final_submission_df = train_prophet_enhancement(initial_submission_df, prophet_weight=prophet_weight_to_use)\n","        # --- END NEW ---\n","\n","        # 5. SAVE FINAL SUBMISSION\n","        final_submission_df.to_csv('submission.csv', index=False)\n","        print(\"\\nSubmission file 'submission.csv' created successfully.\")\n","        \n","        print(\"\\nFirst 5 rows of submission:\")\n","        print(final_submission_df.head())\n","        print(\"\\nLast 5 rows of submission:\")\n","        print(final_submission_df.tail())\n","\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n","        import traceback\n","        traceback.print_exc()\n","\n","    finally:\n","        # 6. CLEANUP\n","        print(\"\\n🧹 Cleaning up variables to free memory...\")\n","        # Check if variables are defined before attempting to delete them\n","        if 'train_full_raw' in locals() and train_full_raw is not None: del train_full_raw\n","        if 'test_full_raw' in locals() and test_full_raw is not None: del test_full_raw\n","        if 'predictor' in locals() and predictor is not None: del predictor\n","        if 'initial_submission_df' in locals() and initial_submission_df is not None: del initial_submission_df\n","        if 'final_submission_df' in locals() and final_submission_df is not None: del final_submission_df\n","        if 'sample_submission' in locals() and sample_submission is not None: del sample_submission # This was the problematic one\n","        gc.collect()\n","        print(\"Cleanup complete. Ready for another run.\")\n","\n","# --- This line calls the main function to start the pipeline ---\n","main()"]},{"cell_type":"code","execution_count":null,"id":"5614d289","metadata":{"papermill":{"duration":0.003685,"end_time":"2025-07-17T15:41:47.592482","exception":false,"start_time":"2025-07-17T15:41:47.588797","status":"completed"},"tags":[]},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"databundleVersionId":12993472,"sourceId":96164,"sourceType":"competition"}],"dockerImageVersionId":31090,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"papermill":{"default_parameters":{},"duration":796.702841,"end_time":"2025-07-17T15:41:50.59381","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2025-07-17T15:28:33.890969","version":"2.6.0"}},"nbformat":4,"nbformat_minor":5}