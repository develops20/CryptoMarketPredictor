{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nicholas33/drw-crypto-market-prediction-cb-lgbm-xgb-nb153?scriptVersionId=250962739\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# Step 1: Installations\n!pip install catboost lightgbm xgboost -q\n\n# Step 2: Imports\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport gc\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.feature_selection import SelectKBest, f_regression\nfrom sklearn.metrics import mean_absolute_error\nfrom scipy.stats import pearsonr\nimport lightgbm as lgb\nimport xgboost as xgb\nimport catboost as cb\nimport time\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:00:12.586838Z","iopub.execute_input":"2025-07-17T06:00:12.58714Z","iopub.status.idle":"2025-07-17T06:00:23.122763Z","shell.execute_reply.started":"2025-07-17T06:00:12.587117Z","shell.execute_reply":"2025-07-17T06:00:23.12192Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Step 3: Class Definition\nclass CryptoMarketPredictor:\n    \"\"\"\n    An optimized pipeline using an ensemble of tree-based models (LGBM, XGBoost, CatBoost)\n    with rich feature engineering to predict crypto market movements.\n    \"\"\"\n    def __init__(self, top_features=100, top_X_features_to_preselect=30):\n        self.top_features = top_features\n        self.top_X_features_to_preselect = top_X_features_to_preselect\n        self.scaler = RobustScaler()\n        self.feature_selector = None\n        self.selected_features = None\n        self.models = {}\n        self.preselected_X_n_names = None\n\n    def optimize_memory(self, df):\n        \"\"\"Optimize DataFrame memory usage.\"\"\"\n        print(f\"Memory usage before optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        for col in df.select_dtypes(include=[np.number]).columns:\n            if col not in ['timestamp', 'ID', 'label']:\n                df[col] = pd.to_numeric(df[col], downcast='float')\n        print(f\"Memory usage after optimization: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n        return df\n\n    def clean_data(self, df):\n        \"\"\"Clean DataFrame by handling inf, NaN values.\"\"\"\n        df.replace([np.inf, -np.inf], np.nan, inplace=True)\n        # Use forward-fill then backward-fill to propagate last valid observation\n        for col in df.columns:\n            if df[col].dtype == 'float32' or df[col].dtype == 'float64':\n                df[col] = df[col].ffill().bfill()\n        df.fillna(0, inplace=True) # Fill any remaining NaNs with 0\n        return df\n\n    def calculate_rsi_proxy(self, prices_series, window=14):\n        \"\"\"Calculate a proxy for the Relative Strength Index (RSI).\"\"\"\n        delta = prices_series.diff()\n        gain = (delta.where(delta > 0, 0)).rolling(window=window, min_periods=1).mean()\n        loss = (-delta.where(delta < 0, 0)).rolling(window=window, min_periods=1).mean()\n        rs = gain / (loss + 1e-10) # Add epsilon to avoid division by zero\n        rsi = 100 - (100 / (1 + rs))\n        return rsi.fillna(50) # Fill initial NaNs with 50 (neutral RSI)\n\n    def create_time_features(self, df):\n        \"\"\"Engineer a rich set of time-based features to capture market memory.\"\"\"\n        print(\"üõ†Ô∏è Engineering rich time-based features...\")\n\n        # Basic price and imbalance features\n        # Ensure these columns exist before trying to access them\n        df['mid_price'] = (df['ask_qty'] + df['bid_qty']) / 2 if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n        df['spread'] = df['ask_qty'] - df['bid_qty'] if 'ask_qty' in df.columns and 'bid_qty' in df.columns else 0.0\n        df['imbalance'] = (df['bid_qty'] - df['ask_qty']) / (df['bid_qty'] + df['ask_qty'] + 1e-10) \\\n                          if 'bid_qty' in df.columns and 'ask_qty' in df.columns else 0.0\n        \n        # Ensure 'volume' column exists before creating rolling features\n        cols_for_rolling = ['mid_price', 'volume', 'imbalance']\n        cols_for_rolling = [col for col in cols_for_rolling if col in df.columns]\n\n        # Rolling window features (MA and STD)\n        rolling_windows = [5, 10, 30, 60]\n        for col in cols_for_rolling:\n            for window in rolling_windows:\n                df[f'{col}_ma_{window}'] = df[col].rolling(window, min_periods=1).mean()\n                df[f'{col}_std_{window}'] = df[col].rolling(window, min_periods=1).std()\n\n        # Lag features\n        lag_periods = [1, 2, 5, 10]\n        cols_for_lags = ['mid_price', 'imbalance']\n        cols_for_lags = [col for col in cols_for_lags if col in df.columns]\n        for col in cols_for_lags:\n            for lag in lag_periods:\n                df[f'{col}_lag_{lag}'] = df[col].shift(lag)\n\n        # Momentum feature - ensure 'mid_price' exists\n        if 'mid_price' in df.columns:\n            df['rsi_proxy_14'] = self.calculate_rsi_proxy(df['mid_price'], window=14)\n        else:\n            df['rsi_proxy_14'] = 50.0 # Default if mid_price is not available\n\n        # Final cleanup to handle NaNs introduced by rolling/shifting\n        df = self.clean_data(df)\n        print(f\"Feature engineering complete. Shape: {df.shape}\")\n        return df\n\n\n    def select_features(self, X_df, y_df):\n        \"\"\"Select top k features using f_regression to reduce dimensionality.\"\"\"\n        print(f\"Selecting top {self.top_features} features from {X_df.shape[1]} features...\")\n        n_features_to_select = min(self.top_features, X_df.shape[1])\n        selector = SelectKBest(score_func=f_regression, k=n_features_to_select)\n        \n        # Handle cases where all target values are the same or features are all NaN/Inf\n        # It's good practice to ensure finite values for f_regression\n        X_df_clean = X_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n        y_df_clean = y_df.replace([np.inf, -np.inf], np.nan).fillna(0)\n\n        # Check for constant target (f_regression will fail)\n        if y_df_clean.nunique() == 1:\n            print(\"Warning: Target variable is constant. Skipping feature selection.\")\n            self.selected_features = X_df_clean.columns.tolist()\n            self.feature_selector = None # No actual selection happened\n            return X_df_clean.values # Return all features\n            \n        selector.fit(X_df_clean, y_df_clean)\n        self.feature_selector = selector\n        self.selected_features = X_df_clean.columns[selector.get_support()].tolist()\n        print(f\"Selected {len(self.selected_features)} features.\")\n        return X_df_clean[self.selected_features].values # Return selected features as numpy array\n\n    def evaluate_model(self, y_true, y_pred, model_name):\n        \"\"\"Evaluate model performance and print results.\"\"\"\n        mae = mean_absolute_error(y_true, y_pred)\n        \n        # Check if y_true or y_pred is constant for Pearsonr\n        if np.std(y_true) == 0 or np.std(y_pred) == 0:\n            correlation = np.nan # Cannot compute correlation if one series is constant\n        else:\n            correlation, _ = pearsonr(y_true, y_pred)\n        \n        print(f\"üìä {model_name} - MAE: {mae:.4f}, Pearson Correlation: {correlation:.4f}\")\n        return correlation\n\n    def train_lightgbm(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train a LightGBM model.\"\"\"\n        print(\"\\nTraining LightGBM model...\")\n        params = {\n            'objective': 'regression_l1', 'metric': 'mae', 'n_estimators': 1500,\n            'learning_rate': 0.03, 'feature_fraction': 0.8, 'bagging_fraction': 0.8,\n            'bagging_freq': 1, 'lambda_l1': 0.1, 'lambda_l2': 0.1,\n            'num_leaves': 31, 'verbose': -1, 'n_jobs': -1, 'seed': 42\n        }\n        model = lgb.LGBMRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)],\n                  callbacks=[lgb.early_stopping(100, verbose=False)])\n        return model\n\n    def train_xgboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train an XGBoost model.\"\"\"\n        print(\"Training XGBoost model...\")\n        params = {\n            'objective': 'reg:squarederror',\n            'eval_metric': 'mae',\n            'n_estimators': 1500,\n            'learning_rate': 0.03,\n            'tree_method': 'hist',\n            'subsample': 0.8,\n            'colsample_bytree': 0.8,\n            'seed': 42,\n            'n_jobs': -1,\n            # 'early_stopping_rounds': 100 # This parameter is for .fit method, not constructor\n        }\n        model = xgb.XGBRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], \n                  early_stopping_rounds=100, verbose=False)\n        return model\n\n    def train_catboost(self, X_train, y_train, X_val, y_val):\n        \"\"\"Train a CatBoost model.\"\"\"\n        print(\"Training CatBoost model...\")\n        params = {\n            'objective': 'MAE', 'eval_metric': 'MAE', 'iterations': 1500,\n            'learning_rate': 0.03, 'random_seed': 42, 'logging_level': 'Silent',\n            'l2_leaf_reg': 3, 'bagging_temperature': 1,\n        }\n        model = cb.CatBoostRegressor(**params)\n        model.fit(X_train, y_train, eval_set=[(X_val, y_val)], early_stopping_rounds=100, verbose=False)\n        return model\n\n    # ---------------------------------FIT METHOD----------------------------------------------------\n    def fit(self, train_data_raw):\n        \"\"\"Main training pipeline.\"\"\"\n        fit_start_time = time.time()\n        print(\"üöÄ Starting training pipeline...\")\n\n        # Ensure 'timestamp' exists and is datetime\n        # Fix: Ensure 'timestamp' column is correctly identified after reset_index\n        if 'timestamp' not in train_data_raw.columns and 'index' in train_data_raw.columns:\n            train_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n        \n        train_data_raw['timestamp'] = pd.to_datetime(train_data_raw['timestamp'])\n\n        # --- 1. Filter data to the most recent 3 months ---\n        print(\"Filtering data to the last 3 months for relevance and speed...\")\n        max_date = train_data_raw['timestamp'].max()\n        three_months_prior = max_date - pd.DateOffset(months=3)\n        train_df = train_data_raw[train_data_raw['timestamp'] >= three_months_prior].copy()\n        print(f\"Training on data from {three_months_prior.date()} to {max_date.date()}. Shape: {train_df.shape}\")\n        del train_data_raw; gc.collect()\n\n        # --- 1a. More intelligent preselection of the anonymous features ---\n        # Use a sample of the data for speed\n        sample_df = train_df.sample(n=min(50000, len(train_df)), random_state=42)\n\n        X_n_cols_raw = [c for c in sample_df.columns if c.startswith('X')]\n\n        X_raw_sample = sample_df[X_n_cols_raw].copy()\n        X_raw_sample.replace([np.inf, -np.inf], np.nan, inplace=True)\n        X_raw_sample.fillna(0, inplace=True)\n\n        y_raw_sample = sample_df['label'].fillna(0)\n\n        # Find the best raw 'X' features from the sample\n        pre_selector = SelectKBest(score_func=f_regression, k=self.top_X_features_to_preselect)\n        \n        # Handle cases where y_raw_sample is constant (f_regression fails)\n        if y_raw_sample.nunique() == 1:\n            print(\"Warning: Sample target variable is constant. Preselecting all 'X' features.\")\n            self.preselected_X_n_names = X_raw_sample.columns.tolist()\n        else:\n            pre_selector.fit(X_raw_sample, y_raw_sample)\n            self.preselected_X_n_names = X_raw_sample.columns[pre_selector.get_support()].tolist()\n\n        print(f\"Found the top {len(self.preselected_X_n_names)} 'X' features: {self.preselected_X_n_names}\")\n        del sample_df, X_raw_sample, y_raw_sample, pre_selector; gc.collect()\n        # --- END OF NEW INTELLIGENT PRE-SELECTION ---\n\n\n        # --- 2. Preprocessing and Feature Engineering ---\n        cols_to_use = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label'] + self.preselected_X_n_names\n        # Filter cols_to_use to only include those that actually exist in train_df\n        cols_to_use = [col for col in cols_to_use if col in train_df.columns]\n\n        print(\"\\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\")\n        print(f\"Using {len(cols_to_use)} columns to generate features\")\n        print(\"-----------------------------------------------------\\n\")\n\n        train_df = train_df[cols_to_use]\n\n        train_df = self.optimize_memory(train_df)\n        train_df = self.create_time_features(train_df)\n\n\n        print(\"\\n--- 2. All Features After Engineering Step ---\")\n        engineered_features_list = train_df.columns.tolist()\n        print(f\"Created {len(engineered_features_list)} total features: {engineered_features_list}\")\n        print(\"------------------------------------------\\n\")\n\n        # --- 3. Feature Selection ---\n        feature_cols = [c for c in train_df.columns if c not in ['timestamp', 'label']]\n        X_df = train_df[feature_cols]\n        y_df = train_df['label']\n\n        X_selected_array = self.select_features(X_df, y_df) # Renamed to avoid confusion with X_selected_df\n        \n        # --- 4. Scaling and Time-Based Validation Split ---\n        print(\"Splitting data into training and validation sets based on time...\")\n        \n        # Use the already fitted scaler, and ensure only selected features are scaled\n        X_scaled = self.scaler.fit_transform(X_selected_array) # Pass the array from select_features\n\n        # Use the last month of data for validation\n        split_date = train_df['timestamp'].max() - pd.DateOffset(months=1)\n        val_start_pos = train_df[train_df['timestamp'] < split_date].shape[0]\n        \n        # Ensure that the split is valid (i.e., val_start_pos < total_rows)\n        if val_start_pos >= X_scaled.shape[0]:\n            print(\"Warning: Validation set is empty due to data filtering/splitting. Using full data for training.\")\n            X_train, y_train = X_scaled, y_df\n            X_val, y_val = X_scaled, y_df # Use train data as val data for basic checks (not ideal for real validation)\n        else:\n            X_train, X_val = X_scaled[:val_start_pos], X_scaled[val_start_pos:]\n            y_train, y_val = y_df.iloc[:val_start_pos], y_df.iloc[val_start_pos:]\n\n        print(f\"Train set size: {X_train.shape[0]}, Validation set size: {X_val.shape[0]}\")\n        del X_df, y_df, X_selected_array, X_scaled, train_df; gc.collect()\n\n\n        # --- 5. Train Models ---\n        self.models['lgb'] = self.train_lightgbm(X_train, y_train, X_val, y_val)\n        self.models['xgb'] = self.train_xgboost(X_train, y_train, X_val, y_val)\n        self.models['cat'] = self.train_catboost(X_train, y_train, X_val, y_val)\n\n        # --- 6. Evaluate Ensemble on Validation Set ---\n        print(\"\\n--- Validation Set Evaluation ---\")\n        val_predictions = {}\n        for name, model in self.models.items():\n            val_predictions[name] = model.predict(X_val)\n            self.evaluate_model(y_val, val_predictions[name], name.upper())\n\n        ensemble_pred = np.mean([val_predictions['lgb'], val_predictions['xgb'], val_predictions['cat']], axis=0)\n        self.evaluate_model(y_val, ensemble_pred, \"ENSEMBLE\")\n\n        print(\"\\n‚úÖ Training pipeline complete.\")\n        return self\n\n\n    def predict(self, test_data_raw):\n        \"\"\"\n        Generate predictions for test data in chunks to manage memory.\n        \"\"\"\n        predict_start_time = time.time()\n        print(\"\\nGenerating predictions in chunks to manage memory...\")\n        if not self.models:\n            raise ValueError(\"Models must be trained first. Call fit().\")\n        if not self.selected_features:\n             raise ValueError(\"Features must be selected first. Call fit().\")\n        if not self.scaler:\n            raise ValueError(\"Scaler must be fitted first. Call fit().\")\n\n\n        chunk_size = 100000\n        num_chunks = int(np.ceil(len(test_data_raw) / chunk_size))\n        all_predictions = []\n        overlap = 60 # Ensure enough data for rolling features\n\n        # Fix: Ensure 'timestamp' is correctly identified in test_data_raw\n        if 'timestamp' not in test_data_raw.columns and 'index' in test_data_raw.columns:\n            test_data_raw.rename(columns={'index': 'timestamp'}, inplace=True)\n\n        for i in range(num_chunks):\n            print(f\"--> Processing chunk {i+1}/{num_chunks}...\")\n\n            start_idx = i * chunk_size\n            end_idx = min((i + 1) * chunk_size, len(test_data_raw))\n            # Adjust start for overlap to ensure rolling features are correctly calculated\n            start_with_overlap = max(0, start_idx - overlap) \n            chunk = test_data_raw.iloc[start_with_overlap:end_idx].copy()\n\n            # Ensure 'timestamp' column is present before converting to datetime\n            if 'timestamp' not in chunk.columns:\n                # Fallback in case of unexpected column naming, although addressed above\n                print(\"Warning: 'timestamp' column not found in chunk. Attempting to use index if available.\")\n                chunk['timestamp'] = chunk.index \n                \n            chunk['timestamp'] = pd.to_datetime(chunk['timestamp'])\n\n            # === Re-evaluated: Removed the redundant 'STEP 1 & STEP 2' comments as they were confusing ===\n            # The logic to identify cols_to_use (base_cols + preselected_X_n_names)\n            # must be robust against columns not existing in test chunk (e.g., if 'buy_qty' isn't there)\n            base_cols = ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume']\n            # Filter cols_to_use to only include those that actually exist in the chunk\n            cols_to_use_in_chunk = [col for col in base_cols + self.preselected_X_n_names if col in chunk.columns]\n            \n            # Ensure at least basic features are present if others are missing\n            if not cols_to_use_in_chunk:\n                raise ValueError(\"No valid columns found in test chunk for feature engineering.\")\n\n            chunk = chunk[cols_to_use_in_chunk]\n\n            chunk_featured = self.create_time_features(chunk)\n\n            # Slice back to the original chunk range AFTER feature engineering\n            # This ensures that rolling/lag features are calculated correctly over the overlap\n            chunk_featured = chunk_featured.iloc[start_idx - start_with_overlap : ]\n\n            # Ensure all selected_features are present in chunk_featured\n            missing_features = [f for f in self.selected_features if f not in chunk_featured.columns]\n            if missing_features:\n                print(f\"Warning: Missing features in test chunk: {missing_features}. Filling with 0.\")\n                for mf in missing_features:\n                    chunk_featured[mf] = 0.0 # Or other appropriate imputation\n\n            X_test_df = chunk_featured[self.selected_features]\n            X_test_scaled = self.scaler.transform(X_test_df)\n            X_test_scaled = np.nan_to_num(X_test_scaled) # Handle any NaNs from scaling if they occur\n\n            chunk_predictions = []\n            for name, model in self.models.items():\n                pred = model.predict(X_test_scaled)\n                chunk_predictions.append(pred)\n\n            ensemble_prediction = np.mean(chunk_predictions, axis=0)\n            all_predictions.append(ensemble_prediction)\n\n        final_predictions = np.concatenate(all_predictions)\n        final_predictions = np.nan_to_num(final_predictions, nan=0.0)\n\n        predict_duration = time.time() - predict_start_time\n        print(f\"Generated {len(final_predictions)} predictions in {predict_duration:.2f} seconds.\")\n        return final_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:10:01.423929Z","iopub.execute_input":"2025-07-17T06:10:01.424335Z","iopub.status.idle":"2025-07-17T06:10:01.462304Z","shell.execute_reply.started":"2025-07-17T06:10:01.424312Z","shell.execute_reply":"2025-07-17T06:10:01.461637Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Cell 3 \ndef main():\n    \"\"\"\n    A wrapper function to define all variables locally and allow for cleanup.\n    \"\"\"\n    print(\"Loading data...\")\n    train_full_raw = None\n    test_full_raw = None\n    predictor = None\n    submission_df = None\n\n    try:\n        # 1. LOAD DATA\n        # Read parquet files. Ensure 'timestamp' is handled if it's the index.\n        # Use 'index=False' if you explicitly want to prevent a column from being created from the index\n        # upon saving the parquet. If it's already an index in the file, then reset_index is needed.\n        \n        # Method 1: Load and explicitly reset index, giving 'timestamp' name\n        # This is often the most robust way if timestamp is the index.\n        train_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/train.parquet')\n        if train_full_raw.index.name == 'timestamp' or 'timestamp' not in train_full_raw.columns:\n            train_full_raw = train_full_raw.reset_index(names='timestamp')\n        \n        test_full_raw = pd.read_parquet('/kaggle/input/drw-crypto-market-prediction/test.parquet')\n        if test_full_raw.index.name == 'timestamp' or 'timestamp' not in test_full_raw.columns:\n            test_full_raw = test_full_raw.reset_index(names='timestamp')\n\n        # Ensure 'ID' column is present for submission\n        # If 'ID' is already a column, this does nothing. Otherwise, it creates it from the default index.\n        if 'ID' not in train_full_raw.columns:\n            train_full_raw['ID'] = train_full_raw.index\n        if 'ID' not in test_full_raw.columns:\n            test_full_raw['ID'] = test_full_raw.index\n\n        print(f\"\\nTrain shape: {train_full_raw.shape}\")\n        print(f\"Test shape: {test_full_raw.shape}\")\n\n        # 2. INITIALIZE AND TRAIN MODEL\n        predictor = CryptoMarketPredictor(top_features=80, top_X_features_to_preselect=25)\n        predictor.fit(train_full_raw)\n\n        # 3. GENERATE PREDICTIONS\n        predictions = predictor.predict(test_full_raw)\n\n        # 4. CREATE SUBMISSION\n        submission_df = pd.DataFrame({'ID': test_full_raw['ID'], 'Prediction': predictions})\n        submission_df.to_csv('submission.csv', index=False)\n        print(\"\\nSubmission file 'submission.csv' created successfully.\")\n        print(submission_df.head())\n\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n        import traceback\n        traceback.print_exc() # Print full traceback for debugging\n\n    finally:\n        # 5. CLEANUP\n        print(\"\\nüßπ Cleaning up variables to free memory...\")\n        del train_full_raw\n        del test_full_raw\n        del predictor\n        del submission_df\n        gc.collect() # Force Python's garbage collector to free up memory\n        print(\"Cleanup complete. Ready for another run.\")\n\n# --- This line calls the main function to start the pipeline ---\nmain()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-17T06:10:06.549229Z","iopub.execute_input":"2025-07-17T06:10:06.549499Z","iopub.status.idle":"2025-07-17T06:11:05.708867Z","shell.execute_reply.started":"2025-07-17T06:10:06.549478Z","shell.execute_reply":"2025-07-17T06:11:05.708243Z"}},"outputs":[{"name":"stdout","text":"Loading data...\n\nTrain shape: (525886, 788)\nTest shape: (538150, 788)\nüöÄ Starting training pipeline...\nFiltering data to the last 3 months for relevance and speed...\nTraining on data from 2023-11-29 to 2024-02-29. Shape: (132178, 788)\nFound the top 25 'X' features: ['X20', 'X21', 'X27', 'X28', 'X29', 'X272', 'X325', 'X425', 'X666', 'X667', 'X670', 'X671', 'X674', 'X675', 'X678', 'X679', 'X750', 'X751', 'X752', 'X758', 'X759', 'X762', 'X766', 'X767', 'X768']\n\n--- 1. Use the intelligent Pre-selected Subset for Engineering ---\nUsing 32 columns to generate features\n-----------------------------------------------------\n\nMemory usage before optimization: 33.28 MB\nMemory usage after optimization: 18.15 MB\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (132178, 68)\n\n--- 2. All Features After Engineering Step ---\nCreated 68 total features: ['timestamp', 'bid_qty', 'ask_qty', 'buy_qty', 'sell_qty', 'volume', 'label', 'X20', 'X21', 'X27', 'X28', 'X29', 'X272', 'X325', 'X425', 'X666', 'X667', 'X670', 'X671', 'X674', 'X675', 'X678', 'X679', 'X750', 'X751', 'X752', 'X758', 'X759', 'X762', 'X766', 'X767', 'X768', 'mid_price', 'spread', 'imbalance', 'mid_price_ma_5', 'mid_price_std_5', 'mid_price_ma_10', 'mid_price_std_10', 'mid_price_ma_30', 'mid_price_std_30', 'mid_price_ma_60', 'mid_price_std_60', 'volume_ma_5', 'volume_std_5', 'volume_ma_10', 'volume_std_10', 'volume_ma_30', 'volume_std_30', 'volume_ma_60', 'volume_std_60', 'imbalance_ma_5', 'imbalance_std_5', 'imbalance_ma_10', 'imbalance_std_10', 'imbalance_ma_30', 'imbalance_std_30', 'imbalance_ma_60', 'imbalance_std_60', 'mid_price_lag_1', 'mid_price_lag_2', 'mid_price_lag_5', 'mid_price_lag_10', 'imbalance_lag_1', 'imbalance_lag_2', 'imbalance_lag_5', 'imbalance_lag_10', 'rsi_proxy_14']\n------------------------------------------\n\nSelecting top 80 features from 66 features...\nSelected 66 features.\nSplitting data into training and validation sets based on time...\nTrain set size: 87560, Validation set size: 44618\n\nTraining LightGBM model...\nTraining XGBoost model...\nTraining CatBoost model...\n\n--- Validation Set Evaluation ---\nüìä LGB - MAE: 0.7239, Pearson Correlation: 0.2052\nüìä XGB - MAE: 0.7308, Pearson Correlation: 0.1410\nüìä CAT - MAE: 0.7232, Pearson Correlation: 0.2243\nüìä ENSEMBLE - MAE: 0.7231, Pearson Correlation: 0.2166\n\n‚úÖ Training pipeline complete.\n\nGenerating predictions in chunks to manage memory...\n--> Processing chunk 1/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (100000, 67)\n--> Processing chunk 2/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 67)\n--> Processing chunk 3/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 67)\n--> Processing chunk 4/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 67)\n--> Processing chunk 5/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (100060, 67)\n--> Processing chunk 6/6...\nüõ†Ô∏è Engineering rich time-based features...\nFeature engineering complete. Shape: (38210, 67)\nGenerated 538150 predictions in 5.17 seconds.\n\nSubmission file 'submission.csv' created successfully.\n   ID  Prediction\n0   0    0.041496\n1   1    0.006678\n2   2   -0.756155\n3   3   -0.064594\n4   4   -0.255123\n\nüßπ Cleaning up variables to free memory...\nCleanup complete. Ready for another run.\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}